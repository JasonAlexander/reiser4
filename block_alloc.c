/* Copyright 2001, 2002 by Hans Reiser, licensing governed by reiser4/README */

#include "debug.h"
#include "dformat.h"
#include "plugin/plugin.h"
#include "txnmgr.h"
#include "znode.h"
#include "block_alloc.h"
#include "tree.h"
#include "super.h"
#include "lib.h"

#include <linux/types.h>	/* for __u??  */
#include <linux/fs.h>		/* for struct super_block  */
#include <linux/spinlock.h>

/* Block number allocation and free space counting in reiser4 are done in two
   stages: first, we assign special block numbers to just created nodes and
   subtracts a number of that nodes from free blocks counter . Those special
   numbers have nothing common with real block numbers on a disk device and
   they are called "fake" in reiser4. Actually only formatted nodes require
   those numbers because we need something to be inserted into internal nodes
   and, thus, to support tree operations: lookup and variants of tree
   modifications. Fake block numbers should be replaced by real "on-disk"
   block numbers before time we write data to disk. It is a second stage of
   block numbers allocation.
  
   Current implementation of reiser4 uses 64-bit integers for block
   numbers. We use highest bit in 64-bit block number to distinguish fake and
   real block numbers. So, only 63 bits may be used to addressing of real
   device blocks. That "fake" block numbers space is divided into subspaces of
   fake block numbers for data blocks and for shadow (working) bitmap
   blocks. Fake block numbers for data blocks are generated by a cyclic
   counter, which gets incremented after each real block allocation. We assume
   that it is impossible to overload this counter during one transaction life.
*/

/* Initialize a blocknr hint. */

void
blocknr_hint_init(reiser4_blocknr_hint * hint)
{
	xmemset(hint, 0, sizeof (reiser4_blocknr_hint));
}

/* Release any resources of a blocknr hint. */
void
blocknr_hint_done(reiser4_blocknr_hint * hint UNUSED_ARG)
{
	/* FIXME: Currently, a blocknr hint has no resources which might be
	   freed */
}

/* is it a real block number from real block device or fake block number for
   not-yet-mapped object? */
/* Audited by: green(2002.06.11) */
int
blocknr_is_fake(const reiser4_block_nr * da)
{
	/* The reason for not simply returning result of '&' operation is that
	   while return value is (possibly 32bit) int,  the reiser4_block_nr is
	   at least 64 bits long, and high bit (which is the only possible
	   non zero bit after the masking) would be stripped off */
	return (*da & REISER4_FAKE_BLOCKNR_BIT_MASK) ? 1 : 0;
}

/* Static functions for <reiser4 super block>/<reiser4 context> block counters
   arithmetic. Mostly, they are isolated to not to code same assertions in
   several places. */

static void
add_to_sb_grabbed(const struct super_block *super, __u64 count)
{
	__u64 grabbed_blocks = reiser4_grabbed_blocks(super);

	grabbed_blocks += count;
	reiser4_set_grabbed_blocks(super, grabbed_blocks);
}

/* Increase the counter of block reserved for flush in super block. */
static void add_to_sb_flush_reserved (const struct super_block * super, __u64 count)
{
	__u64 reserved = reiser4_flush_reserved (super);

	reserved += count;
	reiser4_set_flush_reserved (super, reserved);
}

static void
sub_from_sb_grabbed(const struct super_block *super, __u64 count)
{
	__u64 grabbed_blocks = reiser4_grabbed_blocks(super);
	assert("zam-525", grabbed_blocks >= count);
	grabbed_blocks -= count;
	reiser4_set_grabbed_blocks(super, grabbed_blocks);
}

/* Decrease the counter of block reserved for flush in super block. */
static void sub_from_sb_flush_reserved (const struct super_block * super, __u64 count)
{
	__u64 reserved = reiser4_flush_reserved (super);

	assert ("vpf-291", reserved >= count);
	reserved -= count;
	reiser4_set_flush_reserved (super, reserved);
}

static void
add_to_ctx_grabbed(__u64 count)
{
	reiser4_context *ctx = get_current_context();
	ctx->grabbed_blocks += count;
}

static void
sub_from_ctx_grabbed(__u64 count)
{
	reiser4_context *ctx = get_current_context();

	assert("zam-527", ctx->grabbed_blocks >= count);
	ctx->grabbed_blocks -= count;
}

/* Increase the counter of block reserved for flush in context. */
static void add_to_ctx_flush_reserved (__u64 count)
{
	reiser4_context * ctx = get_current_context();
	ctx->flush_reserved += count;
}

/* Decrease the counter of block reserved for flush in context. */
static void sub_from_ctx_flush_reserved (__u64 count)
{
	reiser4_context * ctx = get_current_context();

	assert ("vpf-284", ctx->flush_reserved >= count);
	ctx->flush_reserved -= count;
}

static void
add_to_sb_unallocated(const struct super_block *super, __u64 count, reiser4_ba_flags_t flags)
{
	__u64 unallocated;

	if (flags & BA_FORMATTED) {
		unallocated = reiser4_fake_allocated(super) + count;
		reiser4_set_fake_allocated(super, unallocated);
	} else {
		unallocated = reiser4_fake_allocated_unformatted(super) + count;
		reiser4_set_fake_allocated_unformatted(super, unallocated);
	}
}

static void
sub_from_sb_unallocated(const struct super_block *super, __u64 count, reiser4_ba_flags_t flags)
{
	__u64 unallocated;

	if (flags & BA_FORMATTED) {
		unallocated = reiser4_fake_allocated(super);
		assert("zam-528", unallocated >= count);
		unallocated -= count;
		reiser4_set_fake_allocated(super, unallocated);
	} else {
		unallocated = reiser4_fake_allocated_unformatted(super);
		assert("zam-528", unallocated >= count);
		unallocated -= count;
		reiser4_set_fake_allocated_unformatted(super, unallocated);
	}
}

static void
add_to_sb_used(const struct super_block *super, __u64 count)
{
	__u64 used_blocks = reiser4_data_blocks(super);
	used_blocks += count;
	reiser4_set_data_blocks(super, used_blocks);
}

static void
sub_from_sb_used(const struct super_block *super, __u64 count)
{
	__u64 used_blocks = reiser4_data_blocks(super);

	assert("zam-530", used_blocks >= count);
	used_blocks -= count;
	reiser4_set_data_blocks(super, used_blocks);
}

/* Increase the counter of block reserved for flush in atom. */
static void add_to_atom_flush_reserved_nolock (txn_atom * atom, __u32 count)
{
	assert ("zam-772", atom != NULL);
	assert ("zam-773", spin_atom_is_locked (atom));
	atom->flush_reserved += count;
}

/* Decrease the counter of block reserved for flush in atom. */
void sub_from_atom_flush_reserved_nolock (txn_atom * atom, __u32 count)
{
	assert ("zam-774", atom != NULL);
	assert ("zam-775", spin_atom_is_locked (atom));
	atom->flush_reserved -= count;
}
/* super block has 4 counters: free, used, grabbed, unallocated. Their sum
   must be number of blocks on a device. This function checks this */
int
check_block_counters(const struct super_block *super)
{
	__u64 sum;

	sum = reiser4_grabbed_blocks(super) + reiser4_free_blocks(super) +
	    	reiser4_data_blocks(super) + reiser4_fake_allocated(super) + 
		reiser4_fake_allocated_unformatted(super) + reiser4_flush_reserved(super);
	if (reiser4_block_count(super) != sum) {
		info("super block counters: "
		     "used %llu, free %llu, "
		     "grabbed %llu, fake allocated (formatetd %llu, unformatted %llu), "
		     "reserved %llu, sum %llu, must be (block count) %llu\n",
		     reiser4_data_blocks(super),
		     reiser4_free_blocks(super),
		     reiser4_grabbed_blocks(super),
		     reiser4_fake_allocated(super),
		     reiser4_fake_allocated_unformatted(super),
		     reiser4_flush_reserved(super),
		     sum, reiser4_block_count(super));
		return 0;
	}
	return 1;
}

/* Get the amount of blocks of 5% of disk. */
reiser4_block_nr reiser4_fs_reserved_space(struct super_block * super) 
{
    return div64_32(reiser4_block_count (super), 20, NULL);
}

void reiser4_grab_space_enable(void) 
{
	get_current_context()->grab_enabled = 1;
}

void reiser4_grab_space_disable(void) 
{
	get_current_context()->grab_enabled = 0;
}

static int reiser4_is_grab_enabled(void)
{
	return get_current_context()->grab_enabled;
}

/* Adjust "working" free blocks counter for number of blocks we are going to
   allocate.  Record number of grabbed blocks in fs-wide and per-thread
   counters.  This function should be called before bitmap scanning or
   allocating fake block numbers
  
   @super           -- pointer to reiser4 super block;
   @min_block_count -- minimum number of blocks we reserve;
   @max_block_count -- maximum number of blocks we want to reserve;
   @reserved        -- out parameter for max. number of reserved blocks, 
                       less than @max_block_count and
                       more than or equal to @min_block_count;
   @return          -- 0 if success,  -ENOSPC, if all
                       free blocks are preserved or already allocated.
*/

/* FIXME-ZAM: reserved blocks could be counted in a reiser4 super block field,
   it allows more error checks. */

static int
reiser4_grab(__u64 * grabbed, __u64 min_block_count, __u64 max_block_count,
	reiser4_ba_flags_t flags)
{
	__u64 free_blocks, reserved_blocks;
	int ret = 0, reserved = flags & BA_RESERVED;
	struct super_block *super = reiser4_get_current_sb();

	reiser4_spin_lock_sb(super);
	
	assert("zam-472", grabbed != NULL);
	assert("zam-474", max_block_count >= min_block_count);
	
	free_blocks = reiser4_free_blocks(super);
	reserved_blocks = reiser4_fs_reserved_space (super);

	trace_on(TRACE_ALLOC, "reiser4_grab: free_blocks %llu\n", free_blocks);

	if ((reserved && (free_blocks < min_block_count)) || 
	    (!reserved && (free_blocks < min_block_count + reserved_blocks))) 
	{
		ret = -ENOSPC;
		
		trace_if(TRACE_ALLOC,
		    info("reiser4_grab: ENOSPC: min %llu, max %llu\n", min_block_count, max_block_count));

		goto unlock_and_ret;
	}

	*grabbed = free_blocks <= max_block_count ? free_blocks : max_block_count;

	add_to_ctx_grabbed(*grabbed);
	add_to_sb_grabbed(super, *grabbed);

	free_blocks -= *grabbed;
	reiser4_set_free_blocks(super, free_blocks);

	check_block_counters(super);

	trace_on(TRACE_ALLOC, "%s: grabbed %llu, free blocks left %llu\n",
		 __FUNCTION__, *grabbed, reiser4_free_blocks (super));

unlock_and_ret:
	reiser4_spin_unlock_sb(super);

	reiser4_grab_space_disable();
	return ret;
}

int
reiser4_grab_space(__u64 * grabbed, __u64 min_block_count, __u64 max_block_count,
	reiser4_ba_flags_t flags)
{
    int ret;

    if (!reiser4_is_grab_enabled())
	    return 0;

    if ((ret = reiser4_grab(grabbed, min_block_count, max_block_count, flags)) == -ENOSPC) {
	    
	    /* Trying to commit the all transactions if BA_CAN_COMMIT flag present */
	    if (flags & BA_CAN_COMMIT) {
		    if (txnmgr_force_commit_all(get_current_context()->super) != 0)
			rpanic("umka-1272", "Can't commit transactions durring block allocation\n");

		if ((ret = reiser4_grab(grabbed, min_block_count, max_block_count, flags)) != 0)
		    return ret;
	    }
    }

    return ret;
}

/* A simple wrapper for reiser4_grab_space, suitable for most places when we
   are going to allocate exact number of blocks .
   Reserved means that allocated from 5% of disk space. */
int
reiser4_grab_space_exact(__u64 count, reiser4_ba_flags_t flags)
{
	__u64 not_used;
	return reiser4_grab_space(&not_used, count, count, flags);
}

/* Grabs space any way and restores grab_enabled flag back */
int
reiser4_grab_space_force(__u64 count, reiser4_ba_flags_t flags)
{
	int ret, save;
   
	save = reiser4_is_grab_enabled();
	reiser4_grab_space_enable();
    
	ret = reiser4_grab_space_exact(count, flags);
    
	if (!save)
		reiser4_grab_space_disable();

	return ret;
}

/* is called after @count fake block numbers are allocated and pointer to
   those blocks are inserted into tree. */
static void
grabbed2fake_allocated(__u64 count, reiser4_ba_flags_t flags)
{
	const struct super_block *super = reiser4_get_current_sb();

	sub_from_ctx_grabbed(count);

	reiser4_spin_lock_sb(super);

	sub_from_sb_grabbed(super, count);
	add_to_sb_unallocated(super, count, flags & BA_FORMATTED);

	assert("vs-922", check_block_counters(super));

	reiser4_spin_unlock_sb(super);
}

/* obtain a block number for new formatted node which will be used to refer
   to this newly allocated node until real allocation is done */
int
assign_fake_blocknr(reiser4_block_nr * blocknr, reiser4_ba_flags_t flags)
{
	static spinlock_t fake_lock = SPIN_LOCK_UNLOCKED;
	static reiser4_block_nr fake_gen = 0;

	spin_lock(&fake_lock);
	*blocknr = fake_gen++;
	spin_unlock(&fake_lock);

	*blocknr &= ~REISER4_BLOCKNR_STATUS_BIT_MASK;
	*blocknr |= REISER4_UNALLOCATED_STATUS_VALUE;

#if REISER4_DEBUG
	{
		znode *node;

		node = zlook(current_tree, blocknr);
		assert("zam-394", node == NULL);
	}
#endif
	trace_on(TRACE_RESERVE, "moving 1 grabbed block to fake allocated.\n");
	grabbed2fake_allocated(1, flags);
	
	return 0;
}

/* adjust sb block counters, if real (on-disk) block allocation immediately
   follows grabbing of free disk space. */
static void
grabbed2used(__u64 count)
{
	const struct super_block *super = reiser4_get_current_sb();

	sub_from_ctx_grabbed(count);

	reiser4_spin_lock_sb(super);

	sub_from_sb_grabbed(super, count);
	add_to_sb_used(super, count);

	assert("nikita-2679", check_block_counters(super));

	reiser4_spin_unlock_sb(super);
}

/* adjust sb block counters when @count unallocated blocks get mapped to disk */
static void
fake_allocated2used(__u64 count, reiser4_ba_flags_t flags)
{
	const struct super_block *super = reiser4_get_current_sb();

	reiser4_spin_lock_sb(super);

	sub_from_sb_unallocated(super, count, flags & BA_FORMATTED);
	add_to_sb_used(super, count);

	assert("nikita-2680", check_block_counters(super));

	reiser4_spin_unlock_sb(super);
}


/* update the per fs  blocknr hint default value. */
void
update_blocknr_hint_default (const struct super_block *s, const reiser4_block_nr * block)
{
	reiser4_super_info_data *private = get_super_private(s);

	reiser4_spin_lock_sb(s);
	if (*block < private->block_count) {
		private->blocknr_hint_default = *block;
	} else {
		warning("zam-676",
			"block number %llu is too large to be used in a blocknr hint\n", (unsigned long long) *block);
	}
	reiser4_spin_unlock_sb(s);
}

/* wrapper to call space allocation plugin */
int
reiser4_alloc_blocks(reiser4_blocknr_hint * hint, reiser4_block_nr * blk, 
	reiser4_block_nr * len, reiser4_ba_flags_t flags)
{
	space_allocator_plugin *splug;
	reiser4_block_nr needed = *len;
	block_stage_t stage = BLOCK_NOT_COUNTED;

	struct super_block *s = reiser4_get_current_sb();

	int ret;

	assert("vpf-339", hint != NULL);
	assert("vs-514", (get_super_private(s) &&
			  get_super_private(s)->space_plug && 
			  get_super_private(s)->space_plug->alloc_blocks));

	trace_on(TRACE_ALLOC,
		 "alloc_blocks: requested %llu, search from %llu\n",
		 (unsigned long long) *len, (unsigned long long) (hint ? hint->blk : ~0ull));
	if (hint != NULL) {
		stage = hint->block_stage;

		/* If blocknr hint isn't set we use per fs "blockr_hint default" */
		/* FIXME-ZAM: should a mount option control this? */
		if (hint->blk == 0) {
			reiser4_spin_lock_sb(s);
			hint->blk = get_super_private(s)->blocknr_hint_default;
			assert("zam-677",
			       hint->blk < get_super_private(s)->block_count);
			reiser4_spin_unlock_sb(s);
		}
	}
	
	/* VITALY: allocator should grab this for internal/tx-lists/similar only. */
	if (hint->block_stage == BLOCK_NOT_COUNTED) {
		trace_on(TRACE_RESERVE, "grab for not counted %llu blocks.\n", *len);
		ret = reiser4_grab_space_force(*len, flags);
		if (ret != 0)
			return ret;
	}

	splug = get_super_private(s)->space_plug;
	ret = splug->alloc_blocks(get_space_allocator(s), hint, (int) needed, blk, len);

	if (!ret) {

		assert("zam-680", *blk < reiser4_block_count(s));
		assert("zam-681", *blk + *len <= reiser4_block_count(s));

		if (flags & BA_PERMANENT) {
			/* we assume that current atom exists at this moment */
			txn_atom * atom = get_current_atom_locked ();
			atom -> nr_blocks_allocated += *len;
			spin_unlock_atom (atom);
		}

		switch (hint->block_stage) {
		case BLOCK_NOT_COUNTED:
		case BLOCK_GRABBED:
			trace_on(TRACE_RESERVE, "use %s %llu blocks.\n",
				hint->block_stage == BLOCK_GRABBED ? 
				"grabbed" : "not counted", *len);
			grabbed2used(*len);
			break;
		case BLOCK_UNALLOCATED:
			trace_on(TRACE_RESERVE, "allocate %llu blocks.\n", *len);
			fake_allocated2used(*len, flags);
			break;
		case BLOCK_FLUSH_RESERVED:
		    	trace_on(TRACE_RESERVE, "get wandered %llu blocks.\n", *len);
			{
				txn_atom * atom = get_current_atom_locked ();
				sub_from_atom_flush_reserved_nolock (atom, *len);
				spin_unlock_atom (atom);
			}
			break;
		default:
			impossible("zam-531", "wrong block stage");
		}
	} else {
		if (hint->block_stage == BLOCK_NOT_COUNTED)
			grabbed2free(needed);
	}

	return ret;
}

/* used -> fake_allocated -> grabbed -> free */

/* adjust sb block counters when @count unallocated blocks get unmapped from
   disk */
static void
used2fake_allocated(__u64 count, reiser4_ba_flags_t flags)
{
	const struct super_block *super = reiser4_get_current_sb();

	reiser4_spin_lock_sb(super);

	add_to_sb_unallocated(super, count, flags & BA_FORMATTED);
	sub_from_sb_used(super, count);

	assert("nikita-2681", check_block_counters(super));

	reiser4_spin_unlock_sb(super);
}

/* disk space, virtually used by fake block numbers is counted as "grabbed" again. */
static void
fake_allocated2grabbed(__u64 count, reiser4_ba_flags_t flags)
{
	const struct super_block *super = reiser4_get_current_sb();

	add_to_ctx_grabbed(count);

	reiser4_spin_lock_sb(super);

	assert("nikita-2682", check_block_counters(super));

	add_to_sb_grabbed(super, count);
	sub_from_sb_unallocated(super, count, flags & BA_FORMATTED);

	assert("nikita-2683", check_block_counters(super));

	reiser4_spin_unlock_sb(super);
}

void
fake_allocated2free(__u64 count, reiser4_ba_flags_t flags)
{
	fake_allocated2grabbed(count, flags);
	grabbed2free(count);
}

/* Adjust free blocks count for blocks which were reserved but were not used. */
void
grabbed2free(__u64 count)
{
	struct super_block *super = reiser4_get_current_sb();

	sub_from_ctx_grabbed(count);

	reiser4_spin_lock_sb(super);

	sub_from_sb_grabbed(super, count);
	reiser4_set_free_blocks(super, reiser4_free_blocks(super) + count);

	assert("nikita-2684", check_block_counters(super));
	reiser4_spin_unlock_sb(super);
}

int check_atom_reserved_blocks(struct txn_atom *atom, __u64 overwrite_set) 
{
    assert ("vpf-288", atom != NULL);

    return atom->flush_reserved >= overwrite_set;
}

void grabbed2flush_reserved_nolock (txn_atom * atom, __u64 count) 
{
	const struct super_block * super = reiser4_get_current_sb(); 

	sub_from_ctx_grabbed (count);

	/* add to atom if exists, otherwise to ctx. */
	if (atom) {
	    atom->flush_reserved += count;
	} else
	    add_to_ctx_flush_reserved (count);

	reiser4_spin_lock_sb(super);

	sub_from_sb_grabbed(super, count);
	add_to_sb_flush_reserved(super, count);

	assert ("vpf-292", check_block_counters (super));

	reiser4_spin_unlock_sb (super);	
}

void grabbed2flush_reserved (__u64 count)
{
	txn_atom * atom = get_current_atom_locked_nocheck ();

	grabbed2flush_reserved_nolock (atom, count);

	if (atom)
		spin_unlock_atom (atom);
}

__u64 reiser4_atom_flush_reserved(void)
{
	__u32 count;
	
	txn_atom * atom = get_current_atom_locked_nocheck ();

	if (!atom) 
	    return 0;

	count = atom->flush_reserved;
	spin_unlock_atom (atom);

	return count;
}

void flush_reserved2free_all ()
{
	struct super_block * super = reiser4_get_current_sb ();
	txn_atom * atom = get_current_atom_locked_nocheck ();
	__u64 count;

	if (!atom) 
	    return;

	count = atom->flush_reserved;
	
	sub_from_atom_flush_reserved_nolock(atom, count);

	reiser4_spin_lock_sb (super);
	
	sub_from_sb_flush_reserved(super, count);
	reiser4_set_free_blocks (super, reiser4_free_blocks (super) + count);
	
	assert ("vpf-277", check_block_counters (super));

	reiser4_spin_unlock_sb (super);
	    
	spin_unlock_atom (atom);
}

void flush_reserved2atom_all_nolock(txn_atom * atom) 
{
	__u32 count = get_current_context()->flush_reserved;

	assert ("zam-771", atom != NULL);
	assert ("zam-770", spin_atom_is_locked (atom));
	
	sub_from_ctx_flush_reserved(count);
	add_to_atom_flush_reserved_nolock(atom, count);
}

/* release all grabbed blocks which where not used. */
void
all_grabbed2free(void)
{
	reiser4_context *ctx = get_current_context();
	__u64 grabbed = ctx->grabbed_blocks;

	grabbed2free(grabbed);
}

/* adjust sb block counters if real (on-disk) blocks do not become unallocated
   after freeing, @count blocks become "grabbed". */
static void
used2grabbed(__u64 count)
{
	const struct super_block *super = reiser4_get_current_sb();

	add_to_ctx_grabbed(count);

	reiser4_spin_lock_sb(super);

	add_to_sb_grabbed(super, count);
	sub_from_sb_used(super, count);

	assert("nikita-2685", check_block_counters(super));

	reiser4_spin_unlock_sb(super);
}

#if REISER4_DEBUG

/* check "allocated" state of given block range */
void
reiser4_check_blocks(const reiser4_block_nr * start, const reiser4_block_nr * len, int desired)
{
	space_allocator_plugin *splug = get_current_super_private()->space_plug;

	assert("zam-625", splug != NULL);

	if (splug->check_blocks != NULL) {
		splug->check_blocks(start, len, desired);
	}
}

/* check "allocated" state of given block */
void
reiser4_check_block(const reiser4_block_nr * block, int desired)
{
	const reiser4_block_nr one = 1;

	reiser4_check_blocks(block, &one, desired);
}

#endif

/* Blocks deallocation function may do an actual deallocation through space
   plugin allocation or store deleted block numbers in atom's delete_set data
   structure depend on @defer parameter. */

/* if BA_DEFER bit is not turned on, @target_stage means the stage of blocks which
   will be deleted from WORKING bitmap. They might be just unmapped from disk, or 
   freed but disk space is still grabbed by current thread, or these blocks must 
   not be counted in any reiser4 sb block counters, see block_stage_t comment */

/* BA_FORMATTED bit is only used when BA_DEFER in not present: it is used to 
 * distinguish blocks allocated for unformatted and formatted nodes */

int
reiser4_dealloc_blocks(const reiser4_block_nr * start, const reiser4_block_nr * len,
		       block_stage_t target_stage, reiser4_ba_flags_t flags)
{
	txn_atom *atom = NULL;
	int ret;

	if (REISER4_DEBUG) {
		struct super_block *s = reiser4_get_current_sb();

		assert("zam-431", *len != 0);
		assert("zam-432", *start != 0);
		assert("zam-558", !blocknr_is_fake(start));

		reiser4_spin_lock_sb(s);
		assert("zam-562", *start < reiser4_block_count(s));
		reiser4_spin_unlock_sb(s);
	}

	if (flags & BA_DEFER) {
		blocknr_set_entry *bsep = NULL;

		/* storing deleted block numbers in a blocknr set
		   datastructure for further actual deletion */
		do {
			atom = get_current_atom_locked();
			assert("zam-430", atom != NULL);

			ret = blocknr_set_add_extent(atom, &atom->delete_set, &bsep, start, len);

			if (ret == -ENOMEM) {
				/* FIXME: JMACD->ZAM: return FAILURE. */
				/* FIXME: ZAM->JMACD: we need a reliable
				   memory allocation for several things
				   including this one. It is used in Linux
				   kernel: see how block heads are
				   allocated */
				return ret;
			}

			/* This loop might spin at most two times */
		} while (ret == -EAGAIN);

		assert("zam-477", ret == 0);
		assert("zam-433", atom != NULL);
		
		spin_unlock_atom(atom);

	} else {
		/* actual deletion is done through space allocator plugin */
		space_allocator_plugin *splug;

		assert("zam-425", get_current_super_private() != NULL);

		splug = get_current_super_private()->space_plug;

		assert("zam-461", splug != NULL);
		assert("zam-462", splug->dealloc_blocks != NULL);

		splug->dealloc_blocks(get_space_allocator(reiser4_get_current_sb()), *start, *len);

		if (flags & BA_PERMANENT) {
			/* These blocks were counted as allocated, we have to revert it
			 * back if allocation is discarded. */
			txn_atom * atom = get_current_atom_locked ();
			atom->nr_blocks_allocated -= *len;
			spin_unlock_atom (atom);
		}

		switch (target_stage) {
		case BLOCK_NOT_COUNTED:
			assert("vs-960", flags & BA_FORMATTED);

			trace_on(TRACE_RESERVE, "moving %llu used blocks to free\n", *len);
			used2grabbed(*len);
			
			/* VITALY: This is what was grabbed for internal/tx-lists/similar only */
			grabbed2free(*len);
			break;
		case BLOCK_GRABBED:
			assert("vs-961", flags & BA_FORMATTED);
			
			trace_on(TRACE_RESERVE, "moving %llu used blocks to grabbed\n", *len);
			used2grabbed(*len);
			break;
		case BLOCK_UNALLOCATED:
			assert("vs-962", !(flags & BA_FORMATTED));
			trace_on(TRACE_RESERVE, "moving %llu used blocks to fake allocated\n", *len);
			used2fake_allocated(*len, flags & BA_FORMATTED);
			break;
		default:
			impossible("zam-532", "wrong block stage");
		}
	}

	return 0;
}

/* wrappers for block allocator plugin methods */
extern void
pre_commit_hook(void)
{
	space_allocator_plugin *splug;

	assert("zam-502", get_current_super_private() != NULL);
	splug = get_current_super_private()->space_plug;
	assert("zam-503", splug != NULL);

	if (splug->pre_commit_hook != NULL)
		splug->pre_commit_hook();
}

/* an actor which applies delete set to block allocator data */
static int
apply_dset(txn_atom * atom UNUSED_ARG, const reiser4_block_nr * a, const reiser4_block_nr * b, void *data UNUSED_ARG)
{
	space_allocator_plugin *splug;
	struct super_block *s = reiser4_get_current_sb();
	reiser4_super_info_data *private = get_super_private(s);

	__u64 len = 1;

	if (b != NULL)
		len = *b;

	if (REISER4_DEBUG) {
		reiser4_spin_lock_sb(s);

		assert("zam-554", *a < reiser4_block_count(s));
		assert("zam-555", *a + len <= reiser4_block_count(s));

		reiser4_spin_unlock_sb(s);
	}

	assert("zam-552", get_current_super_private() != NULL);
	splug = get_current_super_private()->space_plug;
	assert("zam-553", splug != NULL);

	/* it should be safe because of atom stage */
	spin_unlock_atom(atom);

	if (splug->dealloc_blocks != NULL)
		splug->dealloc_blocks(&private->space_allocator, *a, len);

	spin_lock_atom(atom);

	/* adjust sb block counters */
	used2grabbed(len);
	grabbed2free(len);

	return 0;
}

void
post_commit_hook(void)
{
	space_allocator_plugin *splug;
	txn_atom *atom;

	atom = get_current_atom_locked();
	assert("zam-452", atom != NULL);

	/* do the block deallocation which was deferred 
	   until commit is done */
	blocknr_set_iterator(atom, &atom->delete_set, apply_dset, NULL, 1);

	spin_unlock_atom(atom);

	assert("zam-504", get_current_super_private() != NULL);
	splug = get_current_super_private()->space_plug;
	assert("zam-505", splug != NULL);

	if (splug->post_commit_hook != NULL)
		splug->post_commit_hook();
}

void
post_write_back_hook(void)
{
	space_allocator_plugin *splug;

	assert("zam-504", get_current_super_private() != NULL);
	splug = get_current_super_private()->space_plug;
	assert("zam-505", splug != NULL);

	if (splug->post_commit_hook != NULL)
		splug->post_commit_hook();
}

/*
   Local variables:
   c-indentation-style: "K&R"
   mode-name: "LC"
   c-basic-offset: 8
   tab-width: 8
   fill-column: 120
   scroll-step: 1
   End:
*/
