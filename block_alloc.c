/* Copyright 2001, 2002 by Hans Reiser, licensing governed by reiser4/README */

#include "debug.h"
#include "dformat.h"
#include "plugin/plugin.h"
#include "txnmgr.h"
#include "znode.h"
#include "block_alloc.h"
#include "tree.h"
#include "super.h"
#include "lib.h"

#include <linux/types.h>	/* for __u??  */
#include <linux/fs.h>		/* for struct super_block  */
#include <linux/spinlock.h>

/* Block number allocation and free space counting in reiser4 are done in two
   stages: first, we assign special block numbers to just created nodes and
   subtracts a number of those nodes from free blocks counter . Those special
   numbers have nothing common with real block numbers on a disk device and
   they are called "fake" in reiser4. Actually only formatted nodes require
   those numbers because we need something to be inserted into internal nodes
   and, thus, to support tree operations: lookup and variants of tree
   modifications. Fake block numbers should be replaced by real "on-disk"
   block numbers before time we write data to disk. It is a second stage of
   block numbers allocation.
  
   Current implementation of reiser4 uses 64-bit integers for block
   numbers. We use highest bit in 64-bit block number to distinguish fake and
   real block numbers. So, only 63 bits may be used to addressing of real
   device blocks. That "fake" block numbers space is divided into subspaces of
   fake block numbers for data blocks and for shadow (working) bitmap
   blocks. Fake block numbers for data blocks are generated by a cyclic
   counter, which gets incremented after each real block allocation. We assume
   that it is impossible to overload this counter during one transaction life.
*/

/* Initialize a blocknr hint. */

void
blocknr_hint_init(reiser4_blocknr_hint * hint)
{
	xmemset(hint, 0, sizeof (reiser4_blocknr_hint));
}

/* Release any resources of a blocknr hint. */
void
blocknr_hint_done(reiser4_blocknr_hint * hint UNUSED_ARG)
{
	/* FIXME: Currently, a blocknr hint has no resources which might be
	   freed */
}

/* is it a real block number from real block device or fake block number for
   not-yet-mapped object? */
/* Audited by: green(2002.06.11) */
int
blocknr_is_fake(const reiser4_block_nr * da)
{
	/* The reason for not simply returning result of '&' operation is that
	   while return value is (possibly 32bit) int,  the reiser4_block_nr is
	   at least 64 bits long, and high bit (which is the only possible
	   non zero bit after the masking) would be stripped off */
	return (*da & REISER4_FAKE_BLOCKNR_BIT_MASK) ? 1 : 0;
}

/* Static functions for <reiser4 super block>/<reiser4 context> block counters
   arithmetic. Mostly, they are isolated to not to code same assertions in
   several places. */
static void
sub_from_ctx_grabbed(reiser4_context *ctx, __u64 count)
{	
	assert("zam-527", ctx->grabbed_blocks >= count);
	ctx->grabbed_blocks -= count;
}


static void
sub_from_sb_grabbed(reiser4_super_info_data *sbinfo, __u64 count)
{
	assert("zam-525", sbinfo->blocks_grabbed >= count);
	sbinfo->blocks_grabbed -= count;
}

/* Decrease the counter of block reserved for flush in super block. */
static void
sub_from_sb_flush_reserved (reiser4_super_info_data *sbinfo, __u64 count)
{
	assert ("vpf-291", sbinfo->blocks_flush_reserved >= count);
	sbinfo->blocks_flush_reserved -= count;
}

static void
add_to_sb_fake_allocated(reiser4_super_info_data *sbinfo, __u64 count, reiser4_ba_flags_t flags)
{
	if (flags & BA_FORMATTED) {
		sbinfo->blocks_fake_allocated += count;
	} else {
		sbinfo->blocks_fake_allocated_unformatted += count;
	}
}

static void
sub_from_sb_fake_allocated(reiser4_super_info_data *sbinfo, __u64 count, reiser4_ba_flags_t flags)
{
	if (flags & BA_FORMATTED) {
		assert("zam-806", sbinfo->blocks_fake_allocated >= count);
		sbinfo->blocks_fake_allocated -= count;
	} else {
		assert("zam-528", sbinfo->blocks_fake_allocated_unformatted >= count);
		sbinfo->blocks_fake_allocated_unformatted -= count;
	}
}

static void
sub_from_sb_used(reiser4_super_info_data *sbinfo, __u64 count)
{
	assert("zam-530", sbinfo->blocks_used >= count + sbinfo->min_blocks_used);
	sbinfo->blocks_used -= count;
}

/* Increase the counter of block reserved for flush in atom. */
static void
add_to_atom_flush_reserved_nolock (txn_atom * atom, __u32 count)
{
	assert ("zam-772", atom != NULL);
	assert ("zam-773", spin_atom_is_locked (atom));
	atom->flush_reserved += count;
}

/* Decrease the counter of block reserved for flush in atom. */
static void
sub_from_atom_flush_reserved_nolock (txn_atom * atom, __u32 count)
{
	assert ("zam-774", atom != NULL);
	assert ("zam-775", spin_atom_is_locked (atom));
	assert ("nikita-2790", atom->flush_reserved >= count);
	atom->flush_reserved -= count;
}

/* super block has 6 counters: free, used, grabbed, fake allocated (formatted and unformatted) and flush reserved. Their
   sum must be number of blocks on a device. This function checks this */
int
check_block_counters(const struct super_block *super)
{
	__u64 sum;

	sum = reiser4_grabbed_blocks(super) + reiser4_free_blocks(super) +
	    	reiser4_data_blocks(super) + reiser4_fake_allocated(super) + 
		reiser4_fake_allocated_unformatted(super) + flush_reserved(super);
	if (reiser4_block_count(super) != sum) {
		printk("super block counters: "
		       "used %llu, free %llu, "
		       "grabbed %llu, fake allocated (formatetd %llu, unformatted %llu), "
		       "reserved %llu, sum %llu, must be (block count) %llu\n",
		       reiser4_data_blocks(super),
		       reiser4_free_blocks(super),
		       reiser4_grabbed_blocks(super),
		       reiser4_fake_allocated(super),
		       reiser4_fake_allocated_unformatted(super),
		       flush_reserved(super),
		       sum, reiser4_block_count(super));
		return 0;
	}
	return 1;
}

#if REISER4_DEBUG_OUTPUT
void
print_block_counters(const char *prefix, 
		     const struct super_block *super, txn_atom *atom)
{
	if (super == NULL)
		super = reiser4_get_current_sb();
	printk("%s:\tsuper: G: %llu, F: %llu, D: %llu, U: %llu + %llu, R: %llu, T: %llu\n",
	       prefix,
	       reiser4_grabbed_blocks(super),
	       reiser4_free_blocks(super),
	       reiser4_data_blocks(super),
	       reiser4_fake_allocated(super),
	       reiser4_fake_allocated_unformatted(super),
	       flush_reserved(super),
	       reiser4_block_count(super));
	printk("\tcontext: G: %llu",
	       get_current_context()->grabbed_blocks);
	if (atom == NULL)
		atom = get_current_atom_locked_nocheck();
	if (atom != NULL) {
		printk("\tatom: R: %llu", atom->flush_reserved);
		UNLOCK_ATOM(atom);
	}
	printk("\n");
}
#endif

/* Get the amount of blocks of 5% of disk. */
/* FIXME: moved to reiser4_set_block_count */
#if 0
reiser4_block_nr
reiser4_fs_reserved_space(struct super_block * super) 
{
	reiser4_block_nr b;

	b = reiser4_block_count(super);
	/* ZAM-FIXME-HANS: by this we avoid a floating point operation? If that is why, say so. */
	/* 51. / (2^10) == .0498 */
	return (b * 51) >> 10;
}
#endif

/* Adjust "working" free blocks counter for number of blocks we are going to
   allocate.  Record number of grabbed blocks in fs-wide and per-thread
   counters.  This function should be called before bitmap scanning or
   allocating fake block numbers
  
   @super           -- pointer to reiser4 super block;
   @min_block_count -- minimum number of blocks we reserve;
   @max_block_count -- maximum number of blocks we want to reserve;
   @reserved        -- out parameter for max. number of reserved blocks, 
                       less than @max_block_count and
                       more than or equal to @min_block_count;
   @return          -- 0 if success,  -ENOSPC, if all
                       free blocks are preserved or already allocated.
*/

/* FIXME-ZAM: reserved blocks could be counted in a reiser4 super block field,
   it allows more error checks. */

/* ZAM-FIXME-HANS: Is there a coherent account of our space reservation scheme anywhere?  If not, then write it. */

static int
reiser4_grab(reiser4_context *ctx, __u64 count, reiser4_ba_flags_t flags)
{
	__u64 free_blocks;
	int ret = 0, use_reserved = flags & BA_RESERVED;
	reiser4_super_info_data *sbinfo;

	assert("vs-1276", ctx == get_current_context());

	sbinfo = get_super_private(ctx->super);

	reiser4_spin_lock_sb(sbinfo);

	free_blocks = sbinfo->blocks_free;

	trace_on(TRACE_ALLOC, "reiser4_grab: free_blocks %llu\n", free_blocks);

	if ((use_reserved && (free_blocks < count)) || 
	    (!use_reserved && (free_blocks < count + sbinfo->blocks_reserved))) {
		ret = -ENOSPC;
		
		trace_on(TRACE_ALLOC, "reiser4_grab: ENOSPC: count %llu\n", count);

		goto unlock_and_ret;
	}

	ctx->grabbed_blocks += count;

	sbinfo->blocks_grabbed += count;
	sbinfo->blocks_free -= count;

#if REISER4_DEBUG
	ctx->grabbed_initially = count;
	fill_backtrace(ctx->grabbed_at);
#endif

	assert("nikita-2986", check_block_counters(ctx->super));

	trace_on(TRACE_ALLOC, "%s: grabbed %llu, free blocks left %llu\n",
		 __FUNCTION__, count, reiser4_free_blocks (ctx->super));

	/* disable grab space in current context */
	ctx->grab_enabled = 0;

unlock_and_ret:
	reiser4_spin_unlock_sb(sbinfo);

	return RETERR(ret);
}

#if REISER4_TRACE
int
__reiser4_grab_space(__u64 count, reiser4_ba_flags_t flags, const char *message)
#else
int
__reiser4_grab_space(__u64 count, reiser4_ba_flags_t flags)
#endif
{
	int ret;
	reiser4_context *ctx;

	assert("nikita-2964", ergo(flags & BA_CAN_COMMIT, 
				   lock_stack_isclean(get_current_lock_stack())));
	/* NIKITA-FIXME-HANS: change to if_trace_on */
	trace_on(TRACE_RESERVE2, "grab_space: %llu for: %s..", count, message);

	ctx = get_current_context();
	if (!(flags & BA_FORCE) && !is_grab_enabled(ctx)) {
		trace_on(TRACE_RESERVE2, "grab disabled and not forced!\n");
		return 0;
	}

	ret = reiser4_grab(ctx, count, flags);
	if (ret == -ENOSPC) {

		/* Trying to commit the all transactions if BA_CAN_COMMIT flag present */
		if (flags & BA_CAN_COMMIT) {

			trace_on(TRACE_RESERVE2, "force commit!..");

			ret = txnmgr_force_commit_all(ctx->super);
			if (ret != 0)
				reiser4_panic("umka-1272", "Can't commit transactions during block allocation\n");

			ctx->grab_enabled = 1;
			ret = reiser4_grab(ctx, count, flags);
		}
	}
	trace_on(TRACE_RESERVE2, "%s(%d)\n", (ret == 0) ? "ok" : "failed", ret);
	/*
	 * allocation from reserved pool cannot fail. This is severe error.
	 */
	assert("nikita-3005", ergo(flags & BA_RESERVED, ret == 0));
	return ret;
}

/*
 * SPACE RESERVED FOR UNLINK/TRUNCATE
 *
 * Unlink and truncate require space in transaction (to update stat data, at
 * least). But we don't want rm(1) to fail with "No space on device" error.
 *
 * Solution is to reserve 5% of disk space for truncates and
 * unlinks. Specifically, normal space grabbing requests don't grab space from
 * reserved area. Only requests with BA_RESERVED bit in flags are allowed to
 * drain it. Per super block delete_sema semaphore is used to allow only one
 * thread at a time to grab from reserved area.
 *
 * Grabbing from reserved area should always be performed with BA_CAN_COMMIT
 * flag.
 *
 */

#if REISER4_TRACE
int __reiser4_grab_reserved(struct super_block *super,
			    __u64 count, reiser4_ba_flags_t flags, const char *message)
#else
int __reiser4_grab_reserved(struct super_block *super,
			    __u64 count, reiser4_ba_flags_t flags)
#endif
{
	assert("nikita-3175", flags & BA_CAN_COMMIT);

	if (reiser4_grab_space(count, flags, message)) {
		reiser4_super_info_data *sbinfo;

		sbinfo = get_super_private(super);
		down(&sbinfo->delete_sema);
		assert("nikita-2929", sbinfo->delete_sema_owner == NULL);
		sbinfo->delete_sema_owner = current;

		if(reiser4_grab_space(count, flags | BA_RESERVED, message)) {
			warning("zam-833", 
				"reserved space is not enough (%llu, %s)",
				count, message);
			return RETERR(-ENOSPC);
		}
	}
	return 0;
}

void
reiser4_release_reserved(struct super_block *super)
{
	reiser4_super_info_data *info;

	info = get_super_private(super);
	if (info->delete_sema_owner == current) {
		info->delete_sema_owner = NULL;
		up(&info->delete_sema);
	}
}

/* is called after @count fake block numbers are allocated and pointer to
   those blocks are inserted into tree. */
static void
grabbed2fake_allocated_formatted(void)
{
	reiser4_context *ctx;
	reiser4_super_info_data *sbinfo;

	ctx = get_current_context();
	sub_from_ctx_grabbed(ctx, 1);

	sbinfo = get_super_private(ctx->super);
	reiser4_spin_lock_sb(sbinfo);

	sub_from_sb_grabbed(sbinfo, 1);
	sbinfo->blocks_fake_allocated ++;

	assert("vs-922", check_block_counters(ctx->super));

	reiser4_spin_unlock_sb(sbinfo);
}

static void
grabbed2fake_allocated_unformatted(void)
{
	reiser4_context *ctx;
	reiser4_super_info_data *sbinfo;

	ctx = get_current_context();
	sub_from_ctx_grabbed(ctx, 1);

	sbinfo = get_super_private(ctx->super);
	reiser4_spin_lock_sb(sbinfo);

	sub_from_sb_grabbed(sbinfo, 1);
	sbinfo->blocks_fake_allocated_unformatted ++;

	assert("vs-922", check_block_counters(ctx->super));

	reiser4_spin_unlock_sb(sbinfo);
}

static spinlock_t fake_lock = SPIN_LOCK_UNLOCKED;
static reiser4_block_nr fake_gen = 0;

/* obtain a block number for new formatted node which will be used to refer
   to this newly allocated node until real allocation is done */
static inline void assign_fake_blocknr(reiser4_block_nr *blocknr)
{
	spin_lock(&fake_lock);
	*blocknr = fake_gen++;
	spin_unlock(&fake_lock);

	*blocknr &= ~REISER4_BLOCKNR_STATUS_BIT_MASK;
	*blocknr |= REISER4_UNALLOCATED_STATUS_VALUE;
#if REISER4_DEBUG
	{
		znode *node;

		node = zlook(current_tree, blocknr);
		assert("zam-394", node == NULL);
	}
#endif
}

int
assign_fake_blocknr_formatted(reiser4_block_nr *blocknr)
{
	trace_on(TRACE_RESERVE2, "assign_fake_blocknr_formatted: moving 1 grabbed block to fake allocated formatted\n");

	assign_fake_blocknr(blocknr);
	grabbed2fake_allocated_formatted();

	return 0;
}

int
assign_fake_blocknr_unformatted(reiser4_block_nr *blocknr)
{
	trace_on(TRACE_RESERVE2, "assign_fake_blocknr_unformatted: moving 1 grabbed block to fake allocated unformatted\n");

	assign_fake_blocknr(blocknr);
	grabbed2fake_allocated_unformatted();

	return 0;
}


/* adjust sb block counters, if real (on-disk) block allocation immediately
   follows grabbing of free disk space. */
static void
grabbed2used(reiser4_context *ctx, reiser4_super_info_data *sbinfo, __u64 count)
{
	sub_from_ctx_grabbed(ctx, count);

	reiser4_spin_lock_sb(sbinfo);

	sub_from_sb_grabbed(sbinfo, count);
	sbinfo->blocks_used += count;

	assert("nikita-2679", check_block_counters(ctx->super));

	reiser4_spin_unlock_sb(sbinfo);
}

/* adjust sb block counters when @count unallocated blocks get mapped to disk */
static void
fake_allocated2used(reiser4_super_info_data *sbinfo, __u64 count, reiser4_ba_flags_t flags)
{
	reiser4_spin_lock_sb(sbinfo);

	sub_from_sb_fake_allocated(sbinfo, count, flags);
	sbinfo->blocks_used += count;

	assert("nikita-2680", check_block_counters(reiser4_get_current_sb()));

	reiser4_spin_unlock_sb(sbinfo);
}

static void
flush_reserved2used(txn_atom * atom, __u64 count)
{
	reiser4_super_info_data *sbinfo;

	assert("zam-787", atom != NULL);
	assert("zam-788", spin_atom_is_locked(atom));

	sub_from_atom_flush_reserved_nolock(atom, (__u32)count);

	sbinfo = get_current_super_private();
	reiser4_spin_lock_sb(sbinfo);

	sub_from_sb_flush_reserved(sbinfo, count);
	sbinfo->blocks_used += count;

	assert ("zam-789", check_block_counters(reiser4_get_current_sb()));

	reiser4_spin_unlock_sb(sbinfo);	
}

/* update the per fs  blocknr hint default value. */
void
update_blocknr_hint_default (const struct super_block *s, const reiser4_block_nr * block)
{
	reiser4_super_info_data *sbinfo = get_super_private(s);

	reiser4_spin_lock_sb(sbinfo);
	if (*block < sbinfo->block_count) {
		sbinfo->blocknr_hint_default = *block;
	} else {
		warning("zam-676",
			"block number %llu is too large to be used in a blocknr hint\n", (unsigned long long) *block);
	}
	reiser4_spin_unlock_sb(sbinfo);
}

/* wrapper to call space allocation plugin */
int
#if REISER4_TRACE
__reiser4_alloc_blocks(reiser4_blocknr_hint * hint, reiser4_block_nr * blk, 
		       reiser4_block_nr * len, reiser4_ba_flags_t flags, const char *message)
#else
__reiser4_alloc_blocks(reiser4_blocknr_hint * hint, reiser4_block_nr * blk,
		       reiser4_block_nr * len, reiser4_ba_flags_t flags)
#endif
{
	space_allocator_plugin *splug;
	__u64 needed = *len;
	block_stage_t stage = BLOCK_NOT_COUNTED;
	int ret;
	reiser4_context *ctx;
	reiser4_super_info_data *sbinfo;


	ctx = get_current_context();
	sbinfo = get_super_private(ctx->super);
	
	trace_on(TRACE_RESERVE2, "reiser4_alloc_blocks: needed %llu for %s..", needed, message);

	assert("vpf-339", hint != NULL);
	assert("vs-514", (sbinfo && sbinfo->space_plug && sbinfo->space_plug->alloc_blocks));

	trace_on(TRACE_ALLOC,
		 "alloc_blocks: requested %llu, search from %llu\n",
		 (unsigned long long) *len, (unsigned long long) (hint ? hint->blk : ~0ull));
	if (hint != NULL) {
		stage = hint->block_stage;

		/* If blocknr hint isn't set we use per fs "blockr_hint default" */
		/* FIXME-ZAM: should a mount option control this? */
		if (hint->blk == 0) {
			reiser4_spin_lock_sb(sbinfo);
			hint->blk = sbinfo->blocknr_hint_default;
			assert("zam-677",
			       hint->blk < sbinfo->block_count);
			reiser4_spin_unlock_sb(sbinfo);
		}
	}
	
	/* VITALY: allocator should grab this for internal/tx-lists/similar only. */
	if (hint->block_stage == BLOCK_NOT_COUNTED) {
		ret = reiser4_grab_space_force(*len, flags, "ALLOC_BLOCKS for not counted");
		if (ret != 0)
			return ret;
	}

	splug = sbinfo->space_plug;
	/* FIXME: space allocator could be taken right from sbinfo */
	ret = splug->alloc_blocks(get_space_allocator(ctx->super), hint, (int) needed, blk, len);

	if (!ret) {

		assert("zam-680", *blk < reiser4_block_count(ctx->super));
		assert("zam-681", *blk + *len <= reiser4_block_count(ctx->super));

		if (flags & BA_PERMANENT) {
			/* we assume that current atom exists at this moment */
			txn_atom * atom = get_current_atom_locked ();
			atom -> nr_blocks_allocated += *len;
			UNLOCK_ATOM (atom);
		}

		switch (hint->block_stage) {
		case BLOCK_NOT_COUNTED:
		case BLOCK_GRABBED:
			trace_on(TRACE_RESERVE2, "ok. %llu blocks grabbed to used.\n", *len);
			grabbed2used(ctx, sbinfo, *len);
			break;
		case BLOCK_UNALLOCATED:
			trace_on(TRACE_RESERVE2, "ok. %llu blocks fake allocated to used.\n", *len);
			fake_allocated2used(sbinfo, *len, flags);
			break;
		case BLOCK_FLUSH_RESERVED:
			trace_on(TRACE_RESERVE2, "ok. %llu flush reserved to used (get wandered?)\n", *len);
			{
				txn_atom * atom = get_current_atom_locked ();
				flush_reserved2used(atom, *len);
				UNLOCK_ATOM (atom);
			}
			break;
		default:
			impossible("zam-531", "wrong block stage");
		}
	} else {
		assert ("zam-821", ergo(hint->max_dist == 0, ret != -ENOSPC));
		if (hint->block_stage == BLOCK_NOT_COUNTED)
			grabbed2free(ctx, sbinfo, needed, "reiser4_alloc_blocks: failed for BLOCK_NOT_CONTED");
	}

	return ret;
}

/* used -> fake_allocated -> grabbed -> free */

/* adjust sb block counters when @count unallocated blocks get unmapped from
   disk */
static void
used2fake_allocated(reiser4_super_info_data *sbinfo, __u64 count, reiser4_ba_flags_t flags)
{
	reiser4_spin_lock_sb(sbinfo);

	add_to_sb_fake_allocated(sbinfo, count, flags & BA_FORMATTED);

	sub_from_sb_used(sbinfo, count);

	assert("nikita-2681", check_block_counters(reiser4_get_current_sb()));

	reiser4_spin_unlock_sb(sbinfo);
}

static void
used2flush_reserved(reiser4_super_info_data *sbinfo, txn_atom * atom, __u64 count, 
		    reiser4_ba_flags_t flags UNUSED_ARG)
{
	assert("nikita-2791", atom != NULL);
	assert("nikita-2792", spin_atom_is_locked(atom));

	add_to_atom_flush_reserved_nolock(atom, (__u32)count);

	reiser4_spin_lock_sb(sbinfo);

	sbinfo->blocks_flush_reserved += count;
	/*add_to_sb_flush_reserved(sbinfo, count);*/
	sub_from_sb_used(sbinfo, count);

	assert("nikita-2681", check_block_counters(reiser4_get_current_sb()));

	reiser4_spin_unlock_sb(sbinfo);
}

/* disk space, virtually used by fake block numbers is counted as "grabbed" again. */
static void
fake_allocated2grabbed(reiser4_context *ctx, reiser4_super_info_data *sbinfo, __u64 count, reiser4_ba_flags_t flags)
{
	ctx->grabbed_blocks += count;

	reiser4_spin_lock_sb(sbinfo);

	assert("nikita-2682", check_block_counters(ctx->super));

	sbinfo->blocks_grabbed += count;
	sub_from_sb_fake_allocated(sbinfo, count, flags & BA_FORMATTED);

	assert("nikita-2683", check_block_counters(ctx->super));

	reiser4_spin_unlock_sb(sbinfo);
}

void
#if REISER4_TRACE
__fake_allocated2free(__u64 count, reiser4_ba_flags_t flags, const char *message)
#else
__fake_allocated2free(__u64 count, reiser4_ba_flags_t flags)
#endif
{
	reiser4_context *ctx;
	reiser4_super_info_data *sbinfo;

	ctx = get_current_context();
	sbinfo = get_super_private(ctx->super);

	trace_on(TRACE_RESERVE2, "fake_allocated2free %llu blocks for %s\n", count, message);

	fake_allocated2grabbed(ctx, sbinfo, count, flags);
#if REISER4_TRACE
	__grabbed2free(ctx, sbinfo, count, message);
#else
	__grabbed2free(ctx, sbinfo, count);
#endif
}

void grabbed2free_mark(int mark)
{
	reiser4_context *ctx;
	reiser4_super_info_data *sbinfo;

	ctx = get_current_context();
	sbinfo = get_super_private(ctx->super);

	assert("nikita-3007", mark >= 0);
	assert("nikita-3006", 
	       ctx->grabbed_blocks >= (__u64)mark);
	grabbed2free(ctx, sbinfo, ctx->grabbed_blocks - mark, __FUNCTION__);
}

/* Adjust free blocks count for blocks which were reserved but were not used. */
void
#if REISER4_TRACE
__grabbed2free(reiser4_context *ctx, reiser4_super_info_data *sbinfo, __u64 count, const char *message)
#else
__grabbed2free(reiser4_context *ctx, reiser4_super_info_data *sbinfo, __u64 count)
#endif
{
	trace_on(TRACE_RESERVE2, "grabbed2free: %llu for %s\n", count, message);

	sub_from_ctx_grabbed(ctx, count);


	reiser4_spin_lock_sb(sbinfo);

	sub_from_sb_grabbed(sbinfo, count);
	sbinfo->blocks_free += count;
	assert("nikita-2684", check_block_counters(ctx->super));

	reiser4_spin_unlock_sb(sbinfo);
}

int check_atom_reserved_blocks(struct txn_atom *atom, __u64 overwrite_set) 
{
	assert ("vpf-288", atom != NULL);
	
	return atom->flush_reserved >= overwrite_set;
}

void
#if REISER4_TRACE
__grabbed2flush_reserved_nolock(txn_atom * atom, __u64 count, const char *message) 
#else
__grabbed2flush_reserved_nolock(txn_atom * atom, __u64 count) 
#endif
{
	reiser4_context *ctx;
	reiser4_super_info_data *sbinfo;

	assert("vs-1095", atom);

	ctx = get_current_context();
	sbinfo = get_super_private(ctx->super);

	sub_from_ctx_grabbed(ctx, count);

	add_to_atom_flush_reserved_nolock(atom, count);

	reiser4_spin_lock_sb(sbinfo);

	sbinfo->blocks_flush_reserved += count;
	sub_from_sb_grabbed(sbinfo, count);

	assert ("vpf-292", check_block_counters(ctx->super));

	trace_on(TRACE_RESERVE2, "__grabbed2flush_reserved_nolock %llu blocks for %s: atom %u has %llu flush reserved blocks\n",
		 count, message, atom->atom_id, atom->flush_reserved);

	reiser4_spin_unlock_sb(sbinfo);	
}

void
#if REISER4_TRACE
__grabbed2flush_reserved(__u64 count, const char *message)
#else
__grabbed2flush_reserved(__u64 count)
#endif
{
	txn_atom * atom = get_current_atom_locked ();

	trace_on(TRACE_RESERVE2, "__grabbed2flush_reserved for %s\n", message);

	grabbed2flush_reserved_nolock (atom, count, "__grabbed2flush_reserved");

	UNLOCK_ATOM (atom);
}

void flush_reserved2grabbed(txn_atom * atom, __u64 count)
{
	reiser4_context *ctx;
	reiser4_super_info_data *sbinfo;

	assert("nikita-2788", atom != NULL);
	assert("nikita-2789", spin_atom_is_locked(atom));

	ctx = get_current_context();
	sbinfo = get_super_private(ctx->super);

	ctx->grabbed_blocks += count;

	sub_from_atom_flush_reserved_nolock(atom, (__u32)count);

	reiser4_spin_lock_sb(sbinfo);

	sbinfo->blocks_grabbed += count;
	sub_from_sb_flush_reserved(sbinfo, count);

	assert ("vpf-292", check_block_counters (ctx->super));

	reiser4_spin_unlock_sb (sbinfo);	
}

__u64 atom_flush_reserved(void)
{
	__u32 count;
	
	txn_atom * atom = get_current_atom_locked_nocheck ();

	if (!atom) 
	    return 0;

	count = atom->flush_reserved;
	UNLOCK_ATOM (atom);

	return count;
}

void
#if REISER4_TRACE
__flush_reserved2free_all(const char *message)
#else
__flush_reserved2free_all(void)
#endif
{
	reiser4_super_info_data *sbinfo;
	txn_atom * atom = get_current_atom_locked_nocheck ();
	__u64 count;

	if (!atom) 
		return;

	count = atom->flush_reserved;
 
	sub_from_atom_flush_reserved_nolock(atom, (__u32)count);

	sbinfo = get_current_super_private();
	reiser4_spin_lock_sb(sbinfo);
	
	sub_from_sb_flush_reserved(sbinfo, count);
	sbinfo->blocks_free += count;
	
	assert ("vpf-277", check_block_counters(reiser4_get_current_sb()));

	reiser4_spin_unlock_sb(sbinfo);
	    
	trace_on(TRACE_RESERVE2, "flush_reserved2free_all moved %llu flush reserved blocks to free for %s\n", count, message);

	UNLOCK_ATOM (atom);
}

/* release all blocks grabbed in context which where not used. */
void
#if REISER4_TRACE
__all_grabbed2free(const char *message)
#else
__all_grabbed2free(void)
#endif
{
	reiser4_context *ctx = get_current_context();

	grabbed2free(ctx, get_super_private(ctx->super), ctx->grabbed_blocks, message);
}

/* adjust sb block counters if real (on-disk) blocks do not become unallocated
   after freeing, @count blocks become "grabbed". */
static void
used2grabbed(reiser4_context *ctx, reiser4_super_info_data *sbinfo, __u64 count)
{
	ctx->grabbed_blocks += count;

	reiser4_spin_lock_sb(sbinfo);

	sbinfo->blocks_grabbed += count;
	sub_from_sb_used(sbinfo, count);

	assert("nikita-2685", check_block_counters(ctx->super));

	reiser4_spin_unlock_sb(sbinfo);
}

/* this used to be done through used2grabbed and grabbed2free*/
static void
used2free(reiser4_super_info_data *sbinfo, __u64 count)
{
	reiser4_spin_lock_sb(sbinfo);

	sbinfo->blocks_free += count;
	sub_from_sb_used(sbinfo, count);

	assert("nikita-2685", check_block_counters(reiser4_get_current_sb()));

	reiser4_spin_unlock_sb(sbinfo);	
}

#if REISER4_DEBUG

/* check "allocated" state of given block range */
void
reiser4_check_blocks(const reiser4_block_nr * start, const reiser4_block_nr * len, int desired)
{
	space_allocator_plugin *splug = get_current_super_private()->space_plug;

	assert("zam-625", splug != NULL);

	if (splug->check_blocks != NULL) {
		splug->check_blocks(start, len, desired);
	}
}

/* check "allocated" state of given block */
void
reiser4_check_block(const reiser4_block_nr * block, int desired)
{
	const reiser4_block_nr one = 1;

	reiser4_check_blocks(block, &one, desired);
}

#endif

/* Blocks deallocation function may do an actual deallocation through space
   plugin allocation or store deleted block numbers in atom's delete_set data
   structure depend on @defer parameter. */

/* if BA_DEFER bit is not turned on, @target_stage means the stage of blocks which
   will be deleted from WORKING bitmap. They might be just unmapped from disk, or 
   freed but disk space is still grabbed by current thread, or these blocks must 
   not be counted in any reiser4 sb block counters, see block_stage_t comment */

/* BA_FORMATTED bit is only used when BA_DEFER in not present: it is used to 
   distinguish blocks allocated for unformatted and formatted nodes */

int
#if REISER4_TRACE
__reiser4_dealloc_blocks(const reiser4_block_nr * start, const reiser4_block_nr * len,
			 block_stage_t target_stage, reiser4_ba_flags_t flags, const char *message)
#else
__reiser4_dealloc_blocks(const reiser4_block_nr * start, const reiser4_block_nr * len,
			 block_stage_t target_stage, reiser4_ba_flags_t flags)
#endif
{
	txn_atom *atom = NULL;
	int ret;
	reiser4_context *ctx;
	reiser4_super_info_data *sbinfo;

	trace_on(TRACE_RESERVE2, "reiser4_dealloc_blocks: %llu blocks for %s..", *len, message);

	ctx = get_current_context();
	sbinfo = get_super_private(ctx->super);

	if (REISER4_DEBUG) {
		assert("zam-431", *len != 0);
		assert("zam-432", *start != 0);
		assert("zam-558", !blocknr_is_fake(start));

		reiser4_spin_lock_sb(sbinfo);
		assert("zam-562", *start < sbinfo->block_count);
		reiser4_spin_unlock_sb(sbinfo);
	}

	if (flags & BA_DEFER) {
		blocknr_set_entry *bsep = NULL;

		trace_on(TRACE_RESERVE2, "put on delete set\n");

		/* storing deleted block numbers in a blocknr set
		   datastructure for further actual deletion */
		do {
			atom = get_current_atom_locked();
			assert("zam-430", atom != NULL);

			ret = blocknr_set_add_extent(atom, &atom->delete_set, &bsep, start, len);

			if (ret == -ENOMEM) {
				/* FIXME: JMACD->ZAM: return FAILURE. */
				/* FIXME: ZAM->JMACD: we need a reliable
				   memory allocation for several things
				   including this one. It is used in Linux
				   kernel: see how block heads are
				   allocated */
				return ret;
			}

			/* This loop might spin at most two times */
		} while (ret == -EAGAIN);

		assert("zam-477", ret == 0);
		assert("zam-433", atom != NULL);
		
		UNLOCK_ATOM(atom);

	} else {
		/* actual deletion is done through space allocator plugin */
		space_allocator_plugin *splug;

		assert("zam-425", get_current_super_private() != NULL);

		splug = sbinfo->space_plug;

		assert("zam-461", splug != NULL);
		assert("zam-462", splug->dealloc_blocks != NULL);

		splug->dealloc_blocks(get_space_allocator(ctx->super), *start, *len);

		if (flags & BA_PERMANENT) {
			/* These blocks were counted as allocated, we have to revert it
			 * back if allocation is discarded. */
			txn_atom * atom = get_current_atom_locked ();
			atom->nr_blocks_allocated -= *len;
			UNLOCK_ATOM (atom);
		}

		switch (target_stage) {
		case BLOCK_NOT_COUNTED:
			assert("vs-960", flags & BA_FORMATTED);
			
			trace_on(TRACE_RESERVE2, "moved from used to free\n");

			/* VITALY: This is what was grabbed for internal/tx-lists/similar only */
			used2free(sbinfo, *len);
			break;

		case BLOCK_GRABBED:
			
			trace_on(TRACE_RESERVE2, "moved from used to grabbed\n");

			used2grabbed(ctx, sbinfo, *len);
			break;

		case BLOCK_UNALLOCATED:

			trace_on(TRACE_RESERVE2, "moved from used to fake allocated\n");

			used2fake_allocated(sbinfo, *len, flags & BA_FORMATTED);
			break;

		case BLOCK_FLUSH_RESERVED: {
			txn_atom *atom;

			trace_on(TRACE_RESERVE2, "moved from used to flush reserved\n");

			atom = get_current_atom_locked();
			used2flush_reserved(sbinfo, atom, *len, flags & BA_FORMATTED);
			UNLOCK_ATOM(atom);
			break;
		}
		default:
			impossible("zam-532", "wrong block stage");
		}
	}

	return 0;
}

int
#if REISER4_TRACE
__reiser4_dealloc_block(const reiser4_block_nr * block, block_stage_t stage, reiser4_ba_flags_t flags, const char *message)
#else
__reiser4_dealloc_block(const reiser4_block_nr * block, block_stage_t stage, reiser4_ba_flags_t flags)
#endif
{
	const reiser4_block_nr one = 1;
	return reiser4_dealloc_blocks(block, &one, stage, flags | BA_FORMATTED, message);
}

/* wrappers for block allocator plugin methods */
extern void
pre_commit_hook(void)
{
	space_allocator_plugin *splug;

	assert("zam-502", get_current_super_private() != NULL);
	splug = get_current_super_private()->space_plug;
	assert("zam-503", splug != NULL);

	if (splug->pre_commit_hook != NULL)
		splug->pre_commit_hook();
}

/* an actor which applies delete set to block allocator data */
static int
apply_dset(txn_atom * atom UNUSED_ARG, const reiser4_block_nr * a, const reiser4_block_nr * b, void *data UNUSED_ARG)
{
	space_allocator_plugin *splug;
	reiser4_context *ctx;
	reiser4_super_info_data *sbinfo;

	__u64 len = 1;

	ctx = get_current_context();
	sbinfo = get_super_private(ctx->super);

	assert("zam-877", atom->stage >= ASTAGE_PRE_COMMIT);
	assert("zam-552", sbinfo != NULL);
	splug = sbinfo->space_plug;
	assert("zam-553", splug != NULL);

	if (b != NULL)
		len = *b;

	if (REISER4_DEBUG) {
		reiser4_spin_lock_sb(sbinfo);

		assert("zam-554", *a < reiser4_block_count(ctx->super));
		assert("zam-555", *a + len <= reiser4_block_count(ctx->super));

		reiser4_spin_unlock_sb(sbinfo);
	}

	if (splug->dealloc_blocks != NULL)
		splug->dealloc_blocks(&sbinfo->space_allocator, *a, len);

	/* adjust sb block counters */
	used2free(sbinfo, len);

	return 0;
}

void
post_commit_hook(void)
{
	space_allocator_plugin *splug;
	txn_atom *atom;

	atom = get_current_atom_locked();
	assert("zam-452", atom->stage == ASTAGE_POST_COMMIT);
	UNLOCK_ATOM(atom);

	/* do the block deallocation which was deferred 
	   until commit is done */
	blocknr_set_iterator(atom, &atom->delete_set, apply_dset, NULL, 1);

	assert("zam-504", get_current_super_private() != NULL);
	splug = get_current_super_private()->space_plug;
	assert("zam-505", splug != NULL);

	if (splug->post_commit_hook != NULL)
		splug->post_commit_hook();
}

void
post_write_back_hook(void)
{
	space_allocator_plugin *splug;

	assert("zam-504", get_current_super_private() != NULL);
	splug = get_current_super_private()->space_plug;
	assert("zam-505", splug != NULL);

	if (splug->post_commit_hook != NULL)
		splug->post_commit_hook();
}

/*
   Local variables:
   c-indentation-style: "K&R"
   mode-name: "LC"
   c-basic-offset: 8
   tab-width: 8
   fill-column: 120
   scroll-step: 1
   End:
*/
