/* Copyright 2001, 2002 by Hans Reiser, licensing governed by reiser4/README */

#include "debug.h"
#include "dformat.h"
#include "plugin/plugin.h"
#include "txnmgr.h"
#include "znode.h"
#include "block_alloc.h"
#include "tree.h"
#include "super.h"
#include "lib.h"

#include <linux/types.h>	/* for __u??  */
#include <linux/fs.h>		/* for struct super_block  */
#include <linux/spinlock.h>

/* Block number allocation and free space counting in reiser4 are done in two
   stages: first, we assign special block numbers to just created nodes and
   subtracts a number of that nodes from free blocks counter . Those special
   numbers have nothing common with real block numbers on a disk device and
   they are called "fake" in reiser4. Actually only formatted nodes require
   those numbers because we need something to be inserted into internal nodes
   and, thus, to support tree operations: lookup and variants of tree
   modifications. Fake block numbers should be replaced by real "on-disk"
   block numbers before time we write data to disk. It is a second stage of
   block numbers allocation.
  
   Current implementation of reiser4 uses 64-bit integers for block
   numbers. We use highest bit in 64-bit block number to distinguish fake and
   real block numbers. So, only 63 bits may be used to addressing of real
   device blocks. That "fake" block numbers space is divided into subspaces of
   fake block numbers for data blocks and for shadow (working) bitmap
   blocks. Fake block numbers for data blocks are generated by a cyclic
   counter, which gets incremented after each real block allocation. We assume
   that it is impossible to overload this counter during one transaction life.
*/

/* Initialize a blocknr hint. */

void
blocknr_hint_init(reiser4_blocknr_hint * hint)
{
	xmemset(hint, 0, sizeof (reiser4_blocknr_hint));
}

/* Release any resources of a blocknr hint. */
void
blocknr_hint_done(reiser4_blocknr_hint * hint UNUSED_ARG)
{
	/* FIXME: Currently, a blocknr hint has no resources which might be
	   freed */
}

/* is it a real block number from real block device or fake block number for
   not-yet-mapped object? */
/* Audited by: green(2002.06.11) */
int
blocknr_is_fake(const reiser4_block_nr * da)
{
	/* The reason for not simply returning result of '&' operation is that
	   while return value is (possibly 32bit) int,  the reiser4_block_nr is
	   at least 64 bits long, and high bit (which is the only possible
	   non zero bit after the masking) would be stripped off */
	return (*da & REISER4_FAKE_BLOCKNR_BIT_MASK) ? 1 : 0;
}

/* Static functions for <reiser4 super block>/<reiser4 context> block counters
   arithmetic. Mostly, they are isolated to not to code same assertions in
   several places. */

static void
add_to_sb_grabbed(const struct super_block *super, __u64 count)
{
	__u64 grabbed_blocks = reiser4_grabbed_blocks(super);

	grabbed_blocks += count;
	reiser4_set_grabbed_blocks(super, grabbed_blocks);
}

/* Increase the counter of block reserved for flush in super block. */
static void add_to_sb_flush_reserved (const struct super_block * super, __u64 count)
{
	__u64 reserved = flush_reserved (super);

	reserved += count;
	set_flush_reserved (super, reserved);
}

static void
sub_from_sb_grabbed(const struct super_block *super, __u64 count)
{
	__u64 grabbed_blocks = reiser4_grabbed_blocks(super);
	assert("zam-525", grabbed_blocks >= count);
	grabbed_blocks -= count;
	reiser4_set_grabbed_blocks(super, grabbed_blocks);
}

/* Decrease the counter of block reserved for flush in super block. */
static void
sub_from_sb_flush_reserved (const struct super_block * super, __u64 count)
{
	__u64 reserved = flush_reserved (super);

	assert ("vpf-291", reserved >= count);
	reserved -= count;
	set_flush_reserved (super, reserved);
}

static void
add_to_ctx_grabbed(__u64 count)
{
	reiser4_context *ctx = get_current_context();
	ctx->grabbed_blocks += count;
}

static void
sub_from_ctx_grabbed(__u64 count)
{
	reiser4_context *ctx = get_current_context();

	assert("zam-527", ctx->grabbed_blocks >= count);
	ctx->grabbed_blocks -= count;
}

static void
add_to_sb_unallocated(const struct super_block *super, __u64 count, reiser4_ba_flags_t flags)
{
	__u64 unallocated;

	if (flags & BA_FORMATTED) {
		unallocated = reiser4_fake_allocated(super) + count;
		reiser4_set_fake_allocated(super, unallocated);
	} else {
		unallocated = reiser4_fake_allocated_unformatted(super) + count;
		reiser4_set_fake_allocated_unformatted(super, unallocated);
	}
}

static void
sub_from_sb_unallocated(const struct super_block *super, __u64 count, reiser4_ba_flags_t flags)
{
	__u64 unallocated;

	if (flags & BA_FORMATTED) {
		unallocated = reiser4_fake_allocated(super);
		assert("zam-806", unallocated >= count);
		unallocated -= count;
		reiser4_set_fake_allocated(super, unallocated);
	} else {
		unallocated = reiser4_fake_allocated_unformatted(super);
		assert("zam-528", unallocated >= count);
		unallocated -= count;
		reiser4_set_fake_allocated_unformatted(super, unallocated);
	}
}

static void
add_to_sb_used(const struct super_block *super, __u64 count)
{
	__u64 used_blocks = reiser4_data_blocks(super);
	used_blocks += count;
	reiser4_set_data_blocks(super, used_blocks);
}

static void
sub_from_sb_used(const struct super_block *super, __u64 count)
{
	__u64 used_blocks = reiser4_data_blocks(super);

	assert("zam-530", used_blocks >= count + get_super_private(super)->min_blocks_used);
	used_blocks -= count;
	reiser4_set_data_blocks(super, used_blocks);
}

/* Increase the counter of block reserved for flush in atom. */
static void
add_to_atom_flush_reserved_nolock (txn_atom * atom, __u32 count)
{
	assert ("zam-772", atom != NULL);
	assert ("zam-773", spin_atom_is_locked (atom));
	atom->flush_reserved += count;
}

/* Decrease the counter of block reserved for flush in atom. */
static void
sub_from_atom_flush_reserved_nolock (txn_atom * atom, __u32 count)
{
	assert ("zam-774", atom != NULL);
	assert ("zam-775", spin_atom_is_locked (atom));
	assert ("nikita-2790", atom->flush_reserved >= count);
	atom->flush_reserved -= count;
}

/* super block has 6 counters: free, used, grabbed, fake allocated (formatted and unformatted) and flush reserved. Their
   sum must be number of blocks on a device. This function checks this */
int
check_block_counters(const struct super_block *super)
{
	__u64 sum;

	sum = reiser4_grabbed_blocks(super) + reiser4_free_blocks(super) +
	    	reiser4_data_blocks(super) + reiser4_fake_allocated(super) + 
		reiser4_fake_allocated_unformatted(super) + flush_reserved(super);
	if (reiser4_block_count(super) != sum) {
		info("super block counters: "
		     "used %llu, free %llu, "
		     "grabbed %llu, fake allocated (formatetd %llu, unformatted %llu), "
		     "reserved %llu, sum %llu, must be (block count) %llu\n",
		     reiser4_data_blocks(super),
		     reiser4_free_blocks(super),
		     reiser4_grabbed_blocks(super),
		     reiser4_fake_allocated(super),
		     reiser4_fake_allocated_unformatted(super),
		     flush_reserved(super),
		     sum, reiser4_block_count(super));
		return 0;
	}
	return 1;
}

#if REISER4_DEBUG_OUTPUT
void
print_block_counters(const char *prefix, 
		     const struct super_block *super, txn_atom *atom)
{
	if (super == NULL)
		super = reiser4_get_current_sb();
	info("%s:\tsuper: G: %llu, F: %llu, D: %llu, U: %llu + %llu, R: %llu, T: %llu\n",
	     prefix,
	     reiser4_grabbed_blocks(super),
	     reiser4_free_blocks(super),
	     reiser4_data_blocks(super),
	     reiser4_fake_allocated(super),
	     reiser4_fake_allocated_unformatted(super),
	     flush_reserved(super),
	     reiser4_block_count(super));
	info("\tcontext: G: %llu",
	     get_current_context()->grabbed_blocks);
	if (atom == NULL)
		atom = get_current_atom_locked_nocheck();
	if (atom != NULL) {
		info("\tatom: R: %llu", atom->flush_reserved);
		spin_unlock_atom(atom);
	}
	info("\n");
}
#endif

/* Get the amount of blocks of 5% of disk. */
reiser4_block_nr
reiser4_fs_reserved_space(struct super_block * super) 
{
	reiser4_block_nr b;

	b = reiser4_block_count(super);
	/* 51. / (2^10) == .0498 */
	return (b * 51) >> 10;
}

/* Adjust "working" free blocks counter for number of blocks we are going to
   allocate.  Record number of grabbed blocks in fs-wide and per-thread
   counters.  This function should be called before bitmap scanning or
   allocating fake block numbers
  
   @super           -- pointer to reiser4 super block;
   @min_block_count -- minimum number of blocks we reserve;
   @max_block_count -- maximum number of blocks we want to reserve;
   @reserved        -- out parameter for max. number of reserved blocks, 
                       less than @max_block_count and
                       more than or equal to @min_block_count;
   @return          -- 0 if success,  -ENOSPC, if all
                       free blocks are preserved or already allocated.
*/

/* FIXME-ZAM: reserved blocks could be counted in a reiser4 super block field,
   it allows more error checks. */

static int
reiser4_grab(__u64 count, reiser4_ba_flags_t flags)
{
	__u64 free_blocks, reserved_blocks;
	int ret = 0, use_reserved = flags & BA_RESERVED;
	struct super_block *super = reiser4_get_current_sb();

	reiser4_spin_lock_sb(super);
	
	free_blocks = reiser4_free_blocks(super);
	reserved_blocks = reiser4_fs_reserved_space (super);

	trace_on(TRACE_ALLOC, "reiser4_grab: free_blocks %llu\n", free_blocks);

	if ((use_reserved && (free_blocks < count)) || 
	    (!use_reserved && (free_blocks - reserved_blocks < count))) {
		ret = -ENOSPC;
		
		trace_if(TRACE_ALLOC,
		    info("reiser4_grab: ENOSPC: count %llu\n", count));

		goto unlock_and_ret;
	}

	
	add_to_ctx_grabbed(count);
	add_to_sb_grabbed(super, count);

#if REISER4_DEBUG
	get_current_context()->grabbed_initially = count;
	get_current_context()->grabbed_at = __builtin_return_address(0);
#endif

	free_blocks -= count;
	reiser4_set_free_blocks(super, free_blocks);

	assert("nikita-2986", check_block_counters(super));

	trace_on(TRACE_ALLOC, "%s: grabbed %llu, free blocks left %llu\n",
		 __FUNCTION__, count, reiser4_free_blocks (super));

	grab_space_disable();

unlock_and_ret:
	reiser4_spin_unlock_sb(super);

	return ret;
}

#if REISER4_TRACE
int
__reiser4_grab_space(__u64 count, reiser4_ba_flags_t flags, const char *message)
#else
int
__reiser4_grab_space(__u64 count, reiser4_ba_flags_t flags)
#endif
{
    int ret;

    assert("nikita-2964", ergo(flags & BA_CAN_COMMIT, 
			       lock_stack_isclean(get_current_lock_stack())));

    trace_if(TRACE_RESERVE2, info("grab_space: %llu for: %s..", count, message));

    if (!(flags & BA_FORCE) && !is_grab_enabled()) {
	    trace_if(TRACE_RESERVE2, info("grab disabled and not forced!\n"));
	    return 0;
    }

    if ((ret = reiser4_grab(count, flags)) == -ENOSPC) {

	    /* Trying to commit the all transactions if BA_CAN_COMMIT flag present */
	    if (flags & BA_CAN_COMMIT) {

		    trace_if(TRACE_RESERVE2, info("force commit!.."));

		    if (txnmgr_force_commit_all(get_current_context()->super) != 0)
			reiser4_panic("umka-1272", "Can't commit transactions durring block allocation\n");

		    ret = reiser4_grab(count, flags);
	    }
    }
    trace_if(TRACE_RESERVE2, info("%s(%d)\n", (ret == 0) ? "ok" : "failed", ret));
    /*
     * allocation from reserved pool cannot fail. This is severe error.
     */
    assert("nikita-3005", ergo(flags & BA_RESERVED, ret == 0));
    return ret;
}

int
reiser4_grab_reserved(struct super_block *super,
		      __u64 count, reiser4_ba_flags_t flags, const char *message)
{
	reiser4_super_info_data *info;

	info = get_super_private(super);
	if (reiser4_grab_space(count, flags, message)) {
		down(&info->delete_sema);
		assert("nikita-2929", info->delete_sema_owner == NULL);
		info->delete_sema_owner = current;

		if(reiser4_grab_space(count, flags | BA_RESERVED, message)) {
			warning("zam-833", 
				"reserved space is not enough (%llu, %s)",
				count, message);
			return -ENOSPC;
		}
	}
	return 0;
}

void
reiser4_release_reserved(struct super_block *super)
{
	reiser4_super_info_data *info;

	info = get_super_private(super);
	if (info->delete_sema_owner == current) {
		info->delete_sema_owner = NULL;
		/* we have to commit transaction here because we need to get
		   reserved free space back before delete semaphore is up
		   again */
		txnmgr_force_commit_current_atom();
		up(&info->delete_sema);
	}
}

#if 0
/* A simple wrapper for reiser4_grab_space, suitable for most places when we
   are going to allocate exact number of blocks .
   Reserved means that allocated from 5% of disk space. */
int
__reiser4_grab_space_exact(__u64 count, reiser4_ba_flags_t flags
			   , const char *message
	)
{
	__u64 not_used;
	return reiser4_grab_space(&not_used, count, count, flags);
}

/* Grabs space any way and restores grab_enabled flag back */
int
reiser4_grab_space_force(__u64 count, reiser4_ba_flags_t flags)
{
	int ret, save;
   
	save = is_grab_enabled();
	grab_space_enable();
    
	ret = reiser4_grab_space_exact(count, flags);
    
	if (!save)
		grab_space_disable();

	return ret;
}
#endif

/* is called after @count fake block numbers are allocated and pointer to
   those blocks are inserted into tree. */
static void
grabbed2fake_allocated(__u64 count, reiser4_ba_flags_t flags)
{
	const struct super_block *super = reiser4_get_current_sb();

	sub_from_ctx_grabbed(count);

	reiser4_spin_lock_sb(super);

	sub_from_sb_grabbed(super, count);
	add_to_sb_unallocated(super, count, flags);

	assert("vs-922", check_block_counters(super));

	reiser4_spin_unlock_sb(super);
}

static spinlock_t fake_lock = SPIN_LOCK_UNLOCKED;
static reiser4_block_nr fake_gen = 0;

/* obtain a block number for new formatted node which will be used to refer
   to this newly allocated node until real allocation is done */
int
#if REISER4_TRACE
__assign_fake_blocknr(reiser4_block_nr * blocknr, reiser4_ba_flags_t flags, const char *message)
#else
__assign_fake_blocknr(reiser4_block_nr * blocknr, reiser4_ba_flags_t flags)
#endif
{
	spin_lock(&fake_lock);
	*blocknr = fake_gen++;
	spin_unlock(&fake_lock);

	*blocknr &= ~REISER4_BLOCKNR_STATUS_BIT_MASK;
	*blocknr |= REISER4_UNALLOCATED_STATUS_VALUE;

#if REISER4_DEBUG
	{
		znode *node;

		node = zlook(current_tree, blocknr);
		assert("zam-394", node == NULL);
	}
#endif
	trace_on(TRACE_RESERVE2, "assign_fake_blocknr: moving 1 grabbed block to fake allocated for %s\n", message);
	grabbed2fake_allocated((__u64)1, flags);

	return 0;
}

/* adjust sb block counters, if real (on-disk) block allocation immediately
   follows grabbing of free disk space. */
static void
grabbed2used(__u64 count)
{
	const struct super_block *super = reiser4_get_current_sb();

	sub_from_ctx_grabbed(count);

	reiser4_spin_lock_sb(super);

	sub_from_sb_grabbed(super, count);
	add_to_sb_used(super, count);

	assert("nikita-2679", check_block_counters(super));

	reiser4_spin_unlock_sb(super);
}

/* adjust sb block counters when @count unallocated blocks get mapped to disk */
static void
fake_allocated2used(__u64 count, reiser4_ba_flags_t flags)
{
	const struct super_block *super = reiser4_get_current_sb();

	reiser4_spin_lock_sb(super);

	sub_from_sb_unallocated(super, count, flags);
	add_to_sb_used(super, count);

	assert("nikita-2680", check_block_counters(super));

	reiser4_spin_unlock_sb(super);
}


/* update the per fs  blocknr hint default value. */
void
update_blocknr_hint_default (const struct super_block *s, const reiser4_block_nr * block)
{
	reiser4_super_info_data *private = get_super_private(s);

	reiser4_spin_lock_sb(s);
	if (*block < private->block_count) {
		private->blocknr_hint_default = *block;
	} else {
		warning("zam-676",
			"block number %llu is too large to be used in a blocknr hint\n", (unsigned long long) *block);
	}
	reiser4_spin_unlock_sb(s);
}

/* wrapper to call space allocation plugin */
int
#if REISER4_TRACE
__reiser4_alloc_blocks(reiser4_blocknr_hint * hint, reiser4_block_nr * blk, 
		       reiser4_block_nr * len, reiser4_ba_flags_t flags, const char *message)
#else
__reiser4_alloc_blocks(reiser4_blocknr_hint * hint, reiser4_block_nr * blk,
		       reiser4_block_nr * len, reiser4_ba_flags_t flags)
#endif
{
	space_allocator_plugin *splug;
	__u64 needed = *len;
	block_stage_t stage = BLOCK_NOT_COUNTED;
	struct super_block *s = reiser4_get_current_sb();
	int ret;

	trace_if(TRACE_RESERVE2, info("reiser4_alloc_blocks: needed %llu for %s..", needed, message));

	assert("vpf-339", hint != NULL);
	assert("vs-514", (get_super_private(s) &&
			  get_super_private(s)->space_plug && 
			  get_super_private(s)->space_plug->alloc_blocks));

	trace_on(TRACE_ALLOC,
		 "alloc_blocks: requested %llu, search from %llu\n",
		 (unsigned long long) *len, (unsigned long long) (hint ? hint->blk : ~0ull));
	if (hint != NULL) {
		stage = hint->block_stage;

		/* If blocknr hint isn't set we use per fs "blockr_hint default" */
		/* FIXME-ZAM: should a mount option control this? */
		if (hint->blk == 0) {
			reiser4_spin_lock_sb(s);
			hint->blk = get_super_private(s)->blocknr_hint_default;
			assert("zam-677",
			       hint->blk < get_super_private(s)->block_count);
			reiser4_spin_unlock_sb(s);
		}
	}
	
	/* VITALY: allocator should grab this for internal/tx-lists/similar only. */
	if (hint->block_stage == BLOCK_NOT_COUNTED) {
		ret = reiser4_grab_space_force(*len, flags, "ALLOC_BLOCKS for not counted");
		if (ret != 0)
			return ret;
	}

	splug = get_super_private(s)->space_plug;
	ret = splug->alloc_blocks(get_space_allocator(s), hint, (int) needed, blk, len);

	if (!ret) {

		assert("zam-680", *blk < reiser4_block_count(s));
		assert("zam-681", *blk + *len <= reiser4_block_count(s));

		if (flags & BA_PERMANENT) {
			/* we assume that current atom exists at this moment */
			txn_atom * atom = get_current_atom_locked ();
			atom -> nr_blocks_allocated += *len;
			spin_unlock_atom (atom);
		}

		switch (hint->block_stage) {
		case BLOCK_NOT_COUNTED:
		case BLOCK_GRABBED:
			trace_if(TRACE_RESERVE2, info("ok. %llu blocks grabbed to used.\n", *len));
			grabbed2used(*len);
			break;
		case BLOCK_UNALLOCATED:
			trace_if(TRACE_RESERVE2, info("ok. %llu blocks fake allocated to used.\n", *len));
			fake_allocated2used(*len, flags);
			break;
		case BLOCK_FLUSH_RESERVED:
			trace_if(TRACE_RESERVE2, info("ok. %llu flush reserved to used (get wandered?)\n", *len));
			{
				txn_atom * atom = get_current_atom_locked ();
				flush_reserved2used(atom, *len);
				spin_unlock_atom (atom);
			}
			break;
		default:
			impossible("zam-531", "wrong block stage");
		}
	} else {
		assert ("zam-821", ergo(hint->max_dist == 0, ret != -ENOSPC));
		if (hint->block_stage == BLOCK_NOT_COUNTED)
			grabbed2free(needed, "reiser4_alloc_blocks: failed for BLOCK_NOT_CONTED");
	}

	return ret;
}

/* used -> fake_allocated -> grabbed -> free */

/* adjust sb block counters when @count unallocated blocks get unmapped from
   disk */
static void
used2fake_allocated(__u64 count, reiser4_ba_flags_t flags)
{
	const struct super_block *super = reiser4_get_current_sb();

	reiser4_spin_lock_sb(super);

	add_to_sb_unallocated(super, count, flags & BA_FORMATTED);
	sub_from_sb_used(super, count);

	assert("nikita-2681", check_block_counters(super));

	reiser4_spin_unlock_sb(super);
}

static void
used2flush_reserved(txn_atom * atom, __u64 count, 
		    reiser4_ba_flags_t flags UNUSED_ARG)
{
	const struct super_block *super = reiser4_get_current_sb();

	assert("nikita-2791", atom != NULL);
	assert("nikita-2792", spin_atom_is_locked(atom));

	add_to_atom_flush_reserved_nolock(atom, (__u32)count);

	reiser4_spin_lock_sb(super);

	add_to_sb_flush_reserved(super, count);
	sub_from_sb_used(super, count);

	assert("nikita-2681", check_block_counters(super));

	reiser4_spin_unlock_sb(super);
}

/* disk space, virtually used by fake block numbers is counted as "grabbed" again. */
static void
fake_allocated2grabbed(__u64 count, reiser4_ba_flags_t flags)
{
	const struct super_block *super = reiser4_get_current_sb();

	add_to_ctx_grabbed(count);

	reiser4_spin_lock_sb(super);

	assert("nikita-2682", check_block_counters(super));

	add_to_sb_grabbed(super, count);
	sub_from_sb_unallocated(super, count, flags & BA_FORMATTED);

	assert("nikita-2683", check_block_counters(super));

	reiser4_spin_unlock_sb(super);
}

void
#if REISER4_TRACE
__fake_allocated2free(__u64 count, reiser4_ba_flags_t flags, const char *message)
#else
__fake_allocated2free(__u64 count, reiser4_ba_flags_t flags)
#endif
{
	trace_if(TRACE_RESERVE2, info("fake_allocated2free %llu blocks for %s\n", count, message));
	fake_allocated2grabbed(count, flags);
#if REISER4_TRACE
	__grabbed2free(count, message);
#else
	__grabbed2free(count);
#endif
}

void grabbed2free_mark(int mark)
{
	assert("nikita-3007", mark >= 0);
	assert("nikita-3006", get_current_context()->grabbed_blocks >= mark);
	grabbed2free(get_current_context()->grabbed_blocks - mark, __FUNCTION__);
}

/* Adjust free blocks count for blocks which were reserved but were not used. */
void
#if REISER4_TRACE
__grabbed2free(__u64 count, const char *message)
#else
__grabbed2free(__u64 count)
#endif
{
	struct super_block *super = reiser4_get_current_sb();

	trace_if(TRACE_RESERVE2, info("grabbed2free: %llu for %s\n", count, message));

	sub_from_ctx_grabbed(count);

	reiser4_spin_lock_sb(super);

	sub_from_sb_grabbed(super, count);
	reiser4_set_free_blocks(super, reiser4_free_blocks(super) + count);

	assert("nikita-2684", check_block_counters(super));
	reiser4_spin_unlock_sb(super);
}

int check_atom_reserved_blocks(struct txn_atom *atom, __u64 overwrite_set) 
{
	assert ("vpf-288", atom != NULL);
	
	return atom->flush_reserved >= overwrite_set;
}

void
#if REISER4_TRACE
__grabbed2flush_reserved_nolock(txn_atom * atom, __u64 count, const char *message) 
#else
__grabbed2flush_reserved_nolock(txn_atom * atom, __u64 count) 
#endif
{
	const struct super_block * super = reiser4_get_current_sb(); 

	assert("vs-1095", atom);

	sub_from_ctx_grabbed (count);

	add_to_atom_flush_reserved_nolock(atom, count);

	reiser4_spin_lock_sb(super);

	sub_from_sb_grabbed(super, count);
	add_to_sb_flush_reserved(super, count);

	assert ("vpf-292", check_block_counters (super));

	trace_if(TRACE_RESERVE2, info("__grabbed2flush_reserved_nolock %llu blocks for %s: atom %u has %llu flush reserved blocks\n",
				      count, message, atom->atom_id, atom->flush_reserved));

	reiser4_spin_unlock_sb (super);	
}

void
#if REISER4_TRACE
__grabbed2flush_reserved(__u64 count, const char *message)
#else
__grabbed2flush_reserved(__u64 count)
#endif
{
	txn_atom * atom = get_current_atom_locked ();

	trace_if(TRACE_RESERVE2, info("__grabbed2flush_reserved for %s\n", message));

	grabbed2flush_reserved_nolock (atom, count, "__grabbed2flush_reserved");

	spin_unlock_atom (atom);
}

void flush_reserved2grabbed(txn_atom * atom, __u64 count)
{
	struct super_block *super;

	assert("nikita-2788", atom != NULL);
	assert("nikita-2789", spin_atom_is_locked(atom));

	super = reiser4_get_current_sb();

	add_to_ctx_grabbed (count);
	sub_from_atom_flush_reserved_nolock(atom, (__u32)count);

	reiser4_spin_lock_sb(super);

	add_to_sb_grabbed(super, count);
	sub_from_sb_flush_reserved(super, count);

	assert ("vpf-292", check_block_counters (super));

	reiser4_spin_unlock_sb (super);	
}

void flush_reserved2used(txn_atom * atom, __u64 count)
{
	struct super_block *super;

	assert("zam-787", atom != NULL);
	assert("zam-788", spin_atom_is_locked(atom));

	super = reiser4_get_current_sb();

	sub_from_atom_flush_reserved_nolock(atom, (__u32)count);

	reiser4_spin_lock_sb(super);

	add_to_sb_used(super, count);
	sub_from_sb_flush_reserved(super, count);

	assert ("zam-789", check_block_counters (super));

	reiser4_spin_unlock_sb (super);	
}

__u64 atom_flush_reserved(void)
{
	__u32 count;
	
	txn_atom * atom = get_current_atom_locked_nocheck ();

	if (!atom) 
	    return 0;

	count = atom->flush_reserved;
	spin_unlock_atom (atom);

	return count;
}

void
#if REISER4_TRACE
__flush_reserved2free_all(const char *message)
#else
__flush_reserved2free_all(void)
#endif
{
	struct super_block * super = reiser4_get_current_sb ();
	txn_atom * atom = get_current_atom_locked_nocheck ();
	__u64 count;

	if (!atom) 
		return;

	count = atom->flush_reserved;
	
	sub_from_atom_flush_reserved_nolock(atom, (__u32)count);

	reiser4_spin_lock_sb (super);
	
	sub_from_sb_flush_reserved(super, count);
	reiser4_set_free_blocks (super, reiser4_free_blocks (super) + count);
	
	assert ("vpf-277", check_block_counters (super));

	reiser4_spin_unlock_sb (super);
	    
	trace_if(TRACE_RESERVE2, info("flush_reserved2free_all moved %llu flush reserved blocks to free for %s\n", count, message));

	spin_unlock_atom (atom);
}

/* release all blocks grabbed in context which where not used. */
void
#if REISER4_TRACE
__all_grabbed2free(const char *message)
#else
__all_grabbed2free(void)
#endif
{
	reiser4_context *ctx = get_current_context();
	__u64 grabbed = ctx->grabbed_blocks;

	grabbed2free(grabbed, message);
}

/* adjust sb block counters if real (on-disk) blocks do not become unallocated
   after freeing, @count blocks become "grabbed". */
static void
used2grabbed(__u64 count)
{
	const struct super_block *super = reiser4_get_current_sb();

	add_to_ctx_grabbed(count);

	reiser4_spin_lock_sb(super);

	add_to_sb_grabbed(super, count);
	sub_from_sb_used(super, count);

	assert("nikita-2685", check_block_counters(super));

	reiser4_spin_unlock_sb(super);
}

#if REISER4_DEBUG

/* check "allocated" state of given block range */
void
reiser4_check_blocks(const reiser4_block_nr * start, const reiser4_block_nr * len, int desired)
{
	space_allocator_plugin *splug = get_current_super_private()->space_plug;

	assert("zam-625", splug != NULL);

	if (splug->check_blocks != NULL) {
		splug->check_blocks(start, len, desired);
	}
}

/* check "allocated" state of given block */
void
reiser4_check_block(const reiser4_block_nr * block, int desired)
{
	const reiser4_block_nr one = 1;

	reiser4_check_blocks(block, &one, desired);
}

#endif

/* Blocks deallocation function may do an actual deallocation through space
   plugin allocation or store deleted block numbers in atom's delete_set data
   structure depend on @defer parameter. */

/* if BA_DEFER bit is not turned on, @target_stage means the stage of blocks which
   will be deleted from WORKING bitmap. They might be just unmapped from disk, or 
   freed but disk space is still grabbed by current thread, or these blocks must 
   not be counted in any reiser4 sb block counters, see block_stage_t comment */

/* BA_FORMATTED bit is only used when BA_DEFER in not present: it is used to 
   distinguish blocks allocated for unformatted and formatted nodes */

int
#if REISER4_TRACE
__reiser4_dealloc_blocks(const reiser4_block_nr * start, const reiser4_block_nr * len,
			 block_stage_t target_stage, reiser4_ba_flags_t flags, const char *message)
#else
__reiser4_dealloc_blocks(const reiser4_block_nr * start, const reiser4_block_nr * len,
			 block_stage_t target_stage, reiser4_ba_flags_t flags)
#endif
{
	txn_atom *atom = NULL;
	int ret;

	trace_if(TRACE_RESERVE2, info("reiser4_dealloc_blocks: %llu blocks for %s..", *len, message));

	if (REISER4_DEBUG) {
		struct super_block *s = reiser4_get_current_sb();

		assert("zam-431", *len != 0);
		assert("zam-432", *start != 0);
		assert("zam-558", !blocknr_is_fake(start));

		reiser4_spin_lock_sb(s);
		assert("zam-562", *start < reiser4_block_count(s));
		reiser4_spin_unlock_sb(s);
	}

	if (flags & BA_DEFER) {
		blocknr_set_entry *bsep = NULL;

		trace_if(TRACE_RESERVE2, info("put on delete set\n"));

		/* storing deleted block numbers in a blocknr set
		   datastructure for further actual deletion */
		do {
			atom = get_current_atom_locked();
			assert("zam-430", atom != NULL);

			ret = blocknr_set_add_extent(atom, &atom->delete_set, &bsep, start, len);

			if (ret == -ENOMEM) {
				/* FIXME: JMACD->ZAM: return FAILURE. */
				/* FIXME: ZAM->JMACD: we need a reliable
				   memory allocation for several things
				   including this one. It is used in Linux
				   kernel: see how block heads are
				   allocated */
				return ret;
			}

			/* This loop might spin at most two times */
		} while (ret == -EAGAIN);

		assert("zam-477", ret == 0);
		assert("zam-433", atom != NULL);
		
		spin_unlock_atom(atom);

	} else {
		/* actual deletion is done through space allocator plugin */
		space_allocator_plugin *splug;

		assert("zam-425", get_current_super_private() != NULL);

		splug = get_current_super_private()->space_plug;

		assert("zam-461", splug != NULL);
		assert("zam-462", splug->dealloc_blocks != NULL);

		splug->dealloc_blocks(get_space_allocator(reiser4_get_current_sb()), *start, *len);

		if (flags & BA_PERMANENT) {
			/* These blocks were counted as allocated, we have to revert it
			 * back if allocation is discarded. */
			txn_atom * atom = get_current_atom_locked ();
			atom->nr_blocks_allocated -= *len;
			spin_unlock_atom (atom);
		}

		switch (target_stage) {
		case BLOCK_NOT_COUNTED:
			assert("vs-960", flags & BA_FORMATTED);
			
			trace_if(TRACE_RESERVE2, info("moved from used to free\n"));

			used2grabbed(*len);

			/* VITALY: This is what was grabbed for internal/tx-lists/similar only */
			grabbed2free(*len, "reiser4_dealloc_blocks");
			break;

		case BLOCK_GRABBED:
			
			trace_if(TRACE_RESERVE2, info("moved from used to grabbed\n"));

			used2grabbed(*len);
			break;

		case BLOCK_UNALLOCATED:

			trace_if(TRACE_RESERVE2, info("moved from used to fake allocated\n"));

			used2fake_allocated(*len, flags & BA_FORMATTED);
			break;

		case BLOCK_FLUSH_RESERVED: {
			txn_atom *atom;

			trace_if(TRACE_RESERVE2, info("moved from used to flush reserved\n"));

			atom = get_current_atom_locked();
			used2flush_reserved(atom, *len, flags & BA_FORMATTED);
			spin_unlock_atom(atom);
			break;
		}
		default:
			impossible("zam-532", "wrong block stage");
		}
	}

	return 0;
}

int
#if REISER4_TRACE
__reiser4_dealloc_block(const reiser4_block_nr * block, block_stage_t stage, reiser4_ba_flags_t flags, const char *message)
#else
__reiser4_dealloc_block(const reiser4_block_nr * block, block_stage_t stage, reiser4_ba_flags_t flags)
#endif
{
	const reiser4_block_nr one = 1;
	return reiser4_dealloc_blocks(block, &one, stage, flags | BA_FORMATTED, message);
}

/* wrappers for block allocator plugin methods */
extern void
pre_commit_hook(void)
{
	space_allocator_plugin *splug;

	assert("zam-502", get_current_super_private() != NULL);
	splug = get_current_super_private()->space_plug;
	assert("zam-503", splug != NULL);

	if (splug->pre_commit_hook != NULL)
		splug->pre_commit_hook();
}

/* an actor which applies delete set to block allocator data */
static int
apply_dset(txn_atom * atom UNUSED_ARG, const reiser4_block_nr * a, const reiser4_block_nr * b, void *data UNUSED_ARG)
{
	space_allocator_plugin *splug;
	struct super_block *s = reiser4_get_current_sb();
	reiser4_super_info_data *private = get_super_private(s);

	__u64 len = 1;

	if (b != NULL)
		len = *b;

	if (REISER4_DEBUG) {
		reiser4_spin_lock_sb(s);

		assert("zam-554", *a < reiser4_block_count(s));
		assert("zam-555", *a + len <= reiser4_block_count(s));

		reiser4_spin_unlock_sb(s);
	}

	assert("zam-552", get_current_super_private() != NULL);
	splug = get_current_super_private()->space_plug;
	assert("zam-553", splug != NULL);

	/* it should be safe because of atom stage */
	spin_unlock_atom(atom);

	if (splug->dealloc_blocks != NULL)
		splug->dealloc_blocks(&private->space_allocator, *a, len);

	spin_lock_atom(atom);

	/* adjust sb block counters */
	used2grabbed(len);
	grabbed2free(len, "apply dset");

	return 0;
}

void
post_commit_hook(void)
{
	space_allocator_plugin *splug;
	txn_atom *atom;

	atom = get_current_atom_locked();
	assert("zam-452", atom != NULL);

	/* do the block deallocation which was deferred 
	   until commit is done */
	blocknr_set_iterator(atom, &atom->delete_set, apply_dset, NULL, 1);

	spin_unlock_atom(atom);

	assert("zam-504", get_current_super_private() != NULL);
	splug = get_current_super_private()->space_plug;
	assert("zam-505", splug != NULL);

	if (splug->post_commit_hook != NULL)
		splug->post_commit_hook();
}

void
post_write_back_hook(void)
{
	space_allocator_plugin *splug;

	assert("zam-504", get_current_super_private() != NULL);
	splug = get_current_super_private()->space_plug;
	assert("zam-505", splug != NULL);

	if (splug->post_commit_hook != NULL)
		splug->post_commit_hook();
}

/*
   Local variables:
   c-indentation-style: "K&R"
   mode-name: "LC"
   c-basic-offset: 8
   tab-width: 8
   fill-column: 120
   scroll-step: 1
   End:
*/
